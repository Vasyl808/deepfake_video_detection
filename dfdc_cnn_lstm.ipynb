{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasyl808/deepfake_video_detection/blob/main/dfdc_cnn_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjCPksT8rTh4",
        "outputId": "5f06d583-8c3c-4c38-e5a6-7e910f392233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZbPVcJOXZOD3",
        "outputId": "b0419b80-6773-489a-b820-a3879229af82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet_pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (1.26.4)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet_pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (2.32.3)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet_pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet_pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet_pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet_pytorch) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet_pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet_pytorch) (1.3.0)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, facenet_pytorch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 10.4.0\n",
            "    Uninstalling pillow-10.4.0:\n",
            "      Successfully uninstalled pillow-10.4.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet_pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "a80eb0cc85fb4a639b500efdbc5de554"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install facenet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_eOd9pvZVob",
        "outputId": "2d63b40e-30ea-4e3c-f696-6970633cc020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/deepfake-detection-challenge.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sample_submission.csv   \n",
            "replace test_videos/aassnaulhq.mp4? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/deepfake-detection-challenge.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9uEFHpRmX_yK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import os.path\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "import glob\n",
        "import datetime\n",
        "import subprocess\n",
        "from scipy.io import wavfile\n",
        "from facenet_pytorch import MTCNN\n",
        "import shutil\n",
        "\n",
        "\n",
        "class FaceDeepfakeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folders, n_frames=None, n_audio_reads=50027, train=True, device=None, cache_folder='/content/cache'):\n",
        "        \"\"\"n_audio_reads controls the length of the audio sequence: 5000 readings/sec.\"\"\"\n",
        "        self.n_frames = n_frames\n",
        "        self.n_audio_reads = n_audio_reads\n",
        "        self.videos = []\n",
        "        self.train = train\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "        self.cache_folder = cache_folder\n",
        "        self.detector = MTCNN(device='cuda', post_process=False)\n",
        "\n",
        "        # Створюємо директорію кешування, якщо вона не існує\n",
        "        if cache_folder and not os.path.exists(cache_folder):\n",
        "            os.makedirs(cache_folder)\n",
        "\n",
        "        for i in range(len(folders)):\n",
        "            if train:\n",
        "                if i == 0:\n",
        "                    with open('/content/train_sample_videos/metadata.json') as f:\n",
        "                        videos = json.load(f)\n",
        "                        videos = [(os.path.join(folders[i], video), metadata) for (video, metadata) in videos.items()]\n",
        "                        self.videos += videos\n",
        "                else:\n",
        "                    with open(os.path.join(\"/content/test.json\")) as f:\n",
        "                        videos = json.load(f)\n",
        "                        videos = [(os.path.join(folders[i], video), metadata) for (video, metadata) in videos.items()]\n",
        "                        self.videos += videos\n",
        "            else:\n",
        "                self.videos += glob.glob(folders[i] + \"/*.mp4\")\n",
        "\n",
        "    def __process_frame(self, frame, video_id, frame_idx):\n",
        "        cache_path = os.path.join(self.cache_folder, f\"{video_id}_frame{frame_idx}.pt\") if self.cache_folder else None\n",
        "\n",
        "        # Якщо файл з кешем існує, завантажуємо його\n",
        "        if cache_path and os.path.exists(cache_path):\n",
        "            return torch.load(cache_path)\n",
        "\n",
        "        # Інакше обробляємо кадр\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        boxes, _ = self.detector.detect(frame, landmarks=False)\n",
        "\n",
        "        if boxes is None:\n",
        "            return None\n",
        "        else:\n",
        "            box = boxes[0]\n",
        "            width = box[2] - box[0]\n",
        "            height = box[3] - box[1]\n",
        "            expand_x = width * 0.3 / 2\n",
        "            expand_y = height * 0.3 / 2\n",
        "            x1 = max(int(box[0] - expand_x), 0)\n",
        "            y1 = max(int(box[1] - expand_y), 0)\n",
        "            x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "            y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face = cv2.resize(face, (300, 300))\n",
        "            face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            # Зберігаємо оброблений кадр у кеш\n",
        "            if cache_path:\n",
        "                torch.save(face, cache_path)\n",
        "\n",
        "            return face\n",
        "\n",
        "    def __pad_or_trim_frames(self, frames):\n",
        "        if len(frames) == self.n_frames:\n",
        "            return frames\n",
        "        elif len(frames) < self.n_frames:\n",
        "            # Дублюємо кадри, поки не досягнемо необхідної кількості\n",
        "            num_repeats = (self.n_frames) // len(frames) + 1\n",
        "            frames = frames * num_repeats\n",
        "            frames = frames[:self.n_frames]\n",
        "        elif len(frames) > self.n_frames:\n",
        "            # Залишаємо тільки перші 30 кадрів\n",
        "            frames = frames[:self.n_frames]\n",
        "        return frames\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        if self.train:\n",
        "            video, metadata = self.videos[n]\n",
        "        else:\n",
        "            video = self.videos[n]\n",
        "\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        frames = []\n",
        "        frame_idx = 0\n",
        "        while len(frames) < self.n_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            processed_frame = self.__process_frame(frame, video_id, frame_idx)\n",
        "            if processed_frame is not None:\n",
        "                frames.append(processed_frame)\n",
        "            frame_idx += 1\n",
        "\n",
        "        cap.release()\n",
        "        frames = self.__pad_or_trim_frames(frames)\n",
        "        frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "        if self.train:\n",
        "            label = 0.0 if metadata['label'] == 'REAL' else 1.0\n",
        "            return frames, torch.FloatTensor([label]).to(self.device)\n",
        "        else:\n",
        "            return frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QjVO2b1pYoB5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class FaceClassifier(nn.Module):\n",
        "    def __init__(self, n_linear_hidden=256, lstm_hidden_dim=128, num_lstm_layers=1, dropout=0.1):\n",
        "        super(FaceClassifier, self).__init__()\n",
        "\n",
        "        # Завантаження попередньо натренованої EfficientNet\n",
        "        self.cnn = models.efficientnet_b7(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.cnn.children())[:-1])  # Виключаємо шар класифікації\n",
        "\n",
        "        # Розмір виходу від feature extractor\n",
        "        self.feature_output_size = 2560  # EfficientNet B7 дає 2560 ознак\n",
        "\n",
        "        # LSTM для обробки послідовності ознак кожного кадру\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_output_size,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        # Повнозв'язні шари для класифікації\n",
        "        self.fc1 = nn.Linear(2 * lstm_hidden_dim, n_linear_hidden)  # множимо на 2 через bidirectional LSTM\n",
        "        self.fc2 = nn.Linear(n_linear_hidden, 1)\n",
        "\n",
        "    def forward(self, vid_frames):\n",
        "        # Витягання ознак для кожного кадру\n",
        "        batch_size, num_frames, channels, height, width = vid_frames.shape\n",
        "        vid_frames = vid_frames.view(batch_size * num_frames, channels, height, width)\n",
        "\n",
        "        # Використовуємо фічерний екстрактор\n",
        "        with torch.no_grad():\n",
        "            vid_features = self.feature_extractor(vid_frames)\n",
        "\n",
        "        # Переформатовуємо ознаки для LSTM\n",
        "        vid_features = vid_features.view(batch_size, num_frames, -1)  # (batch_size, num_frames, feature_output_size)\n",
        "\n",
        "        # Обробка послідовності кадрів через LSTM\n",
        "        lstm_out, _ = self.lstm(vid_features)  # lstm_out: (batch_size, num_frames, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Використання середнього значення по кадрам для об'єднання послідовності (можна також використовувати останній кадр або інші методи агрегації)\n",
        "        lstm_out = torch.mean(lstm_out, dim=1)  # (batch_size, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Класифікаційні шари\n",
        "        x = torch.relu(self.fc1(lstm_out))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heOrqKqzY29p",
        "outputId": "cc9bd62e-4db6-4787-bd57-657c62d09d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/train_sample_videos', '/content/test_videos'], <class 'list'>\n",
            "all test folders: ['/content/train_sample_videos', '/content/test_videos'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
            "100%|██████████| 255M/255M [00:02<00:00, 132MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 14:35:40.834613\n",
            "using device: cuda\n",
            "100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [1:00:26, 36.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[448  29]\n",
            " [ 16 307]]\n",
            "Classification Report (Train Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.94      0.95       477\n",
            "        Fake       0.91      0.95      0.93       323\n",
            "\n",
            "    accuracy                           0.94       800\n",
            "   macro avg       0.94      0.94      0.94       800\n",
            "weighted avg       0.94      0.94      0.94       800\n",
            "\n",
            "\n",
            "epoch: 0, train loss: 0.14641888292972, executed in: 3626.3428843021393\n",
            "train total: 800, train correct: 755.0, train incorrect: 45.0, train accuracy: 0.9437499642372131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [20:39, 12.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[455  22]\n",
            " [ 12 311]]\n",
            "Classification Report (Train Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.95      0.96       477\n",
            "        Fake       0.93      0.96      0.95       323\n",
            "\n",
            "    accuracy                           0.96       800\n",
            "   macro avg       0.95      0.96      0.96       800\n",
            "weighted avg       0.96      0.96      0.96       800\n",
            "\n",
            "\n",
            "epoch: 1, train loss: 0.1102919635293074, executed in: 1239.8142051696777\n",
            "train total: 800, train correct: 766.0, train incorrect: 34.0, train accuracy: 0.9574999809265137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [20:29, 12.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[462  15]\n",
            " [ 15 308]]\n",
            "Classification Report (Train Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.97      0.97       477\n",
            "        Fake       0.95      0.95      0.95       323\n",
            "\n",
            "    accuracy                           0.96       800\n",
            "   macro avg       0.96      0.96      0.96       800\n",
            "weighted avg       0.96      0.96      0.96       800\n",
            "\n",
            "\n",
            "epoch: 2, train loss: 0.08927351533668115, executed in: 1230.0214631557465\n",
            "train total: 800, train correct: 770.0, train incorrect: 30.0, train accuracy: 0.9624999761581421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [20:41, 12.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[458  19]\n",
            " [ 11 312]]\n",
            "Classification Report (Train Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.98      0.96      0.97       477\n",
            "        Fake       0.94      0.97      0.95       323\n",
            "\n",
            "    accuracy                           0.96       800\n",
            "   macro avg       0.96      0.96      0.96       800\n",
            "weighted avg       0.96      0.96      0.96       800\n",
            "\n",
            "\n",
            "epoch: 3, train loss: 0.10506657025427557, executed in: 1242.0170331001282\n",
            "train total: 800, train correct: 770.0, train incorrect: 30.0, train accuracy: 0.9624999761581421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [20:51, 12.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[460  17]\n",
            " [ 12 311]]\n",
            "Classification Report (Train Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.96      0.97       477\n",
            "        Fake       0.95      0.96      0.96       323\n",
            "\n",
            "    accuracy                           0.96       800\n",
            "   macro avg       0.96      0.96      0.96       800\n",
            "weighted avg       0.96      0.96      0.96       800\n",
            "\n",
            "\n",
            "epoch: 4, train loss: 0.07326189557847101, executed in: 1251.4552285671234\n",
            "train total: 800, train correct: 771.0, train incorrect: 29.0, train accuracy: 0.9637500047683716\n",
            "end time: 2024-11-06 16:58:56.990695\n",
            "executed in: 2:23:16.156082\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.6 GB\n",
            "Cached:    0.8 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = ['/content/train_sample_videos', '/content/test_videos']\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = ['/content/train_sample_videos', '/content/test_videos']\n",
        "\n",
        "batch_size = int(8)\n",
        "num_epochs = int(5)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/4_classifier_30_small.pt', map_location=device))\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "#test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    model.train()\n",
        "    torch.cuda.empty_cache()\n",
        "    for i, batch in tqdm(enumerate(train_loader)):\n",
        "        # if i * batch_size >= epoch_size:\n",
        "        #     break\n",
        "        video_data, labels = batch\n",
        "        video_data = video_data.to(device)\n",
        "        #audio_data = audio_data.to(device)\n",
        "\n",
        "        output = model(video_data)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.round()\n",
        "\n",
        "        n_wrong = (labels - output).abs().sum()\n",
        "        t_count_wrong += n_wrong\n",
        "        t_count += labels.shape[0]\n",
        "\n",
        "        epoch_t_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_labels_all.extend(labels.cpu().detach().numpy())\n",
        "        train_preds_all.extend(output.cpu().detach().numpy())\n",
        "\n",
        "        #print('.', end='', flush=True)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Перетворіть на numpy-масиви\n",
        "    train_labels_all = np.array(train_labels_all).astype(int)\n",
        "    train_preds_all = np.array(train_preds_all).astype(int)\n",
        "\n",
        "    # Обчисліть та виведіть матрицю плутанини для тренувального набору\n",
        "    conf_matrix_train = confusion_matrix(train_labels_all, train_preds_all)\n",
        "    print(conf_matrix_train)\n",
        "    # plt.figure(figsize=(8, 6))\n",
        "    #sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    #plt.xlabel('Predicted Label')\n",
        "    #plt.ylabel('True Label')\n",
        "    #plt.title('Confusion Matrix (Train Set)')\n",
        "    #plt.show()\n",
        "\n",
        "    # Виведіть класифікаційний звіт для тренувального набору\n",
        "    print(\"Classification Report (Train Set):\")\n",
        "    print(classification_report(train_labels_all, train_preds_all, target_names=['Real', 'Fake']))\n",
        "\n",
        "    #all_labels = []\n",
        "    #all_preds = []\n",
        "\n",
        "    #model.eval()\n",
        "    #with torch.no_grad():\n",
        "    #    v_count = 0\n",
        "    #    v_count_wrong = 0\n",
        "    #    for i, batch in enumerate(val_loader):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "   #         video_data, labels = batch\n",
        "   #         video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "  #          output = model(video_data)\n",
        "  #          loss = criterion(output, labels)\n",
        "\n",
        "            #output = torch.sigmoid(output)\n",
        "            #output = output.round()\n",
        "            #n_wrong = (labels - output).abs().sum()\n",
        "            #v_count_wrong += n_wrong\n",
        "            #v_count += labels.shape[0]\n",
        "\n",
        "            #epoch_v_loss += loss.item()\n",
        "\n",
        "            #all_labels.extend(labels.cpu().numpy())\n",
        "            #all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    train_losses.append(epoch_t_loss/len(train_loader))\n",
        "    #val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    t_count_right = t_count - t_count_wrong\n",
        "    #v_count_right = v_count - v_count_wrong\n",
        "    t_accuracy = t_count_right / t_count\n",
        "    #v_accuracy = v_count_right / v_count\n",
        "\n",
        "    train_accuracies.append(t_accuracy)\n",
        "    #val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, train loss: {train_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    #print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    #all_labels = np.array(all_labels).astype(int)\n",
        "    #all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    #conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    #plt.figure(figsize=(8, 6))\n",
        "    #sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    #plt.xlabel('Predicted Label')\n",
        "    #plt.ylabel('True Label')\n",
        "    #plt.title('Confusion Matrix')\n",
        "    #plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    #print(\"Classification Report:\")\n",
        "    #print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "    #scheduler.step()\n",
        "    ### Saving model per best validation loss\n",
        "    if best_loss > train_losses[-1]:\n",
        "        best_loss = train_losses[-1]\n",
        "        end_time = datetime.datetime.now()\n",
        "        torch.save(model.state_dict(), f'/content/drive/MyDrive/{epoch}_classifier_{n_frames}_small.pt')\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "#df = pd.DataFrame()\n",
        "#df['train_loss'] = train_losses.cpu().numpy()\n",
        "#df['val_loss'] = val_losses.cpu().numpy()\n",
        "#df['train_acc'] = train_accuracies.cpu().numpy()\n",
        "#df['val_acc'] = val_accuracies.cpu().numpy()\n",
        "#df['epoch_times'] = epoch_times\n",
        "\n",
        "#df.to_csv(f'train_classifier_nframes{n_frames}_bs{batch_size}_lr{lr}.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt')"
      ],
      "metadata": {
        "id": "pXx_SQy6a5T6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive_uadfv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vapCtPi7Evg",
        "outputId": "b95a84e3-1102-42f9-d072-c15c9e27749b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive_uadfv.zip\n",
            "  inflating: fake_videos/test/fake/0046_fake.mp4  \n",
            "  inflating: fake_videos/test/fake/0047_fake.mp4  \n",
            "  inflating: fake_videos/test/fake/0048_fake.mp4  \n",
            "  inflating: fake_videos/test/real/0046.mp4  \n",
            "  inflating: fake_videos/test/real/0047.mp4  \n",
            "  inflating: fake_videos/test/real/0048.mp4  \n",
            "  inflating: fake_videos/train/fake/0000_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0001_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0002_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0003_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0004_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0005_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0006_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0007_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0008_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0009_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0010_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0011_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0012_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0013_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0014_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0015_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0016_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0017_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0018_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0019_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0020_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0021_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0022_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0023_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0024_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0025_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0026_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0027_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0028_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0029_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0030_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0031_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0032_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0033_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0034_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0035_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0036_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0037_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0038_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0039_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0040_fake.mp4  \n",
            "  inflating: fake_videos/train/real/0000.mp4  \n",
            "  inflating: fake_videos/train/real/0001.mp4  \n",
            "  inflating: fake_videos/train/real/0002.mp4  \n",
            "  inflating: fake_videos/train/real/0003.mp4  \n",
            "  inflating: fake_videos/train/real/0004.mp4  \n",
            "  inflating: fake_videos/train/real/0005.mp4  \n",
            "  inflating: fake_videos/train/real/0006.mp4  \n",
            "  inflating: fake_videos/train/real/0007.mp4  \n",
            "  inflating: fake_videos/train/real/0008.mp4  \n",
            "  inflating: fake_videos/train/real/0009.mp4  \n",
            "  inflating: fake_videos/train/real/0010.mp4  \n",
            "  inflating: fake_videos/train/real/0011.mp4  \n",
            "  inflating: fake_videos/train/real/0012.mp4  \n",
            "  inflating: fake_videos/train/real/0013.mp4  \n",
            "  inflating: fake_videos/train/real/0014.mp4  \n",
            "  inflating: fake_videos/train/real/0015.mp4  \n",
            "  inflating: fake_videos/train/real/0016.mp4  \n",
            "  inflating: fake_videos/train/real/0017.mp4  \n",
            "  inflating: fake_videos/train/real/0018.mp4  \n",
            "  inflating: fake_videos/train/real/0019.mp4  \n",
            "  inflating: fake_videos/train/real/0020.mp4  \n",
            "  inflating: fake_videos/train/real/0021.mp4  \n",
            "  inflating: fake_videos/train/real/0022.mp4  \n",
            "  inflating: fake_videos/train/real/0023.mp4  \n",
            "  inflating: fake_videos/train/real/0024.mp4  \n",
            "  inflating: fake_videos/train/real/0025.mp4  \n",
            "  inflating: fake_videos/train/real/0026.mp4  \n",
            "  inflating: fake_videos/train/real/0027.mp4  \n",
            "  inflating: fake_videos/train/real/0028.mp4  \n",
            "  inflating: fake_videos/train/real/0029.mp4  \n",
            "  inflating: fake_videos/train/real/0030.mp4  \n",
            "  inflating: fake_videos/train/real/0031.mp4  \n",
            "  inflating: fake_videos/train/real/0032.mp4  \n",
            "  inflating: fake_videos/train/real/0033.mp4  \n",
            "  inflating: fake_videos/train/real/0034.mp4  \n",
            "  inflating: fake_videos/train/real/0035.mp4  \n",
            "  inflating: fake_videos/train/real/0036.mp4  \n",
            "  inflating: fake_videos/train/real/0037.mp4  \n",
            "  inflating: fake_videos/train/real/0038.mp4  \n",
            "  inflating: fake_videos/train/real/0040.mp4  \n",
            "  inflating: fake_videos/train/real/0041.mp4  \n",
            "  inflating: fake_videos/validation/fake/0041_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0042_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0043_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0044_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0045_fake.mp4  \n",
            "  inflating: fake_videos/validation/real/0039.mp4  \n",
            "  inflating: fake_videos/validation/real/0042.mp4  \n",
            "  inflating: fake_videos/validation/real/0043.mp4  \n",
            "  inflating: fake_videos/validation/real/0044.mp4  \n",
            "  inflating: fake_videos/validation/real/0045.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceDeepfakeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folders, n_frames=None, n_audio_reads=50027, train=True, device=None, cache_folder=None):\n",
        "        \"\"\"n_audio_reads controls the length of the audio sequence: 5000 readings/sec.\"\"\"\n",
        "        self.n_frames = n_frames\n",
        "        self.n_audio_reads = n_audio_reads\n",
        "        self.videos = []\n",
        "        self.train = train\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "        self.cache_folder = cache_folder\n",
        "        self.detector = MTCNN(device='cpu', post_process=False)\n",
        "        for i in range(len(folders)):\n",
        "            if train:\n",
        "                #print(folders[i])\n",
        "                for item in os.listdir(folders[i]):\n",
        "                    item_path = os.path.join(folders[i], item)\n",
        "                    if str(os.path.basename(item_path)) == 'fake':\n",
        "                        for curr in os.listdir(item_path):\n",
        "                            if os.path.isfile(os.path.join(item_path, curr)):\n",
        "                                metadata = {}\n",
        "                                metadata['label']='fake'\n",
        "                                self.videos += [(os.path.join(item_path, curr), metadata)]\n",
        "                    else:\n",
        "                        for item_file in os.listdir(item_path):\n",
        "                            if os.path.isfile(os.path.join(item_path, item_file)):\n",
        "                                metadata = {}\n",
        "                                metadata['label']='real'\n",
        "                                self.videos += [(os.path.join(item_path, item_file), metadata)]\n",
        "            else:\n",
        "                self.videos += glob.glob(folders[i] + \"/*.mp4\")\n",
        "\n",
        "    def __process_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\n",
        "\n",
        "        if boxes is None:\n",
        "            # Повертаємо порожній кадр, якщо обличчя не знайдено\n",
        "            face = torch.zeros((3, 300, 300))\n",
        "        else:\n",
        "            # Беремо перше обличчя (якщо знайдено кілька)\n",
        "            box = boxes[0]\n",
        "\n",
        "            # Розширюємо рамку на 30%\n",
        "            width = box[2] - box[0]\n",
        "            height = box[3] - box[1]\n",
        "            expand_x = width * 0.3 / 2\n",
        "            expand_y = height * 0.3 / 2\n",
        "            x1 = max(int(box[0] - expand_x), 0)\n",
        "            y1 = max(int(box[1] - expand_y), 0)\n",
        "            x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "            y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "            # Обрізаємо зображення за новими координатами\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\n",
        "\n",
        "            # Перетворюємо на тензор і нормалізуємо\n",
        "            face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return face\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        if self.train:\n",
        "            video, metadata = self.videos[n]\n",
        "        else:\n",
        "            video = self.videos[n]\n",
        "\n",
        "        # Processing video frames\n",
        "        if os.path.islink(video):\n",
        "            video = os.readlink(video)\n",
        "\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        frames = []\n",
        "        for _ in range(self.n_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(self.__process_frame(frame))\n",
        "        '''total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step = max(1, total_frames // self.n_frames)\n",
        "\n",
        "        for i in range(self.n_frames):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frames.append(self.__process_frame(frame))\n",
        "            else:\n",
        "                break'''\n",
        "        cap.release()\n",
        "        frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "        # Return data\n",
        "        if self.train:\n",
        "            label = 0.0\n",
        "            if metadata['label'] == 'fake':\n",
        "                label = 1.0\n",
        "            return frames, torch.FloatTensor([label]).to(self.device)\n",
        "        else:\n",
        "            return frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "\n",
        "'''def __process_frame(self, frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\n",
        "\n",
        "    if boxes is None:\n",
        "        # Повертаємо None, якщо обличчя не знайдено\n",
        "        return None\n",
        "    else:\n",
        "        # Беремо перше обличчя (якщо знайдено кілька)\n",
        "        box = boxes[0]\n",
        "\n",
        "        # Розширюємо рамку на 30%\n",
        "        width = box[2] - box[0]\n",
        "        height = box[3] - box[1]\n",
        "        expand_x = width * 0.3 / 2\n",
        "        expand_y = height * 0.3 / 2\n",
        "        x1 = max(int(box[0] - expand_x), 0)\n",
        "        y1 = max(int(box[1] - expand_y), 0)\n",
        "        x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "        y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "        # Обрізаємо зображення за новими координатами\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "        face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\n",
        "\n",
        "        # Перетворюємо на тензор і нормалізуємо\n",
        "        face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return face\n",
        "\n",
        "def __getitem__(self, n):\n",
        "    if self.train:\n",
        "        video, metadata = self.videos[n]\n",
        "    else:\n",
        "        video = self.videos[n]\n",
        "\n",
        "    # Processing video frames\n",
        "    if os.path.islink(video):\n",
        "        video = os.readlink(video)\n",
        "\n",
        "    cap = cv2.VideoCapture(video)\n",
        "\n",
        "    frames = []\n",
        "    while len(frames) < self.n_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            # Якщо дійшли до кінця відео, повертаємось на початок\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "            continue\n",
        "\n",
        "        processed_frame = self.__process_frame(frame)\n",
        "        if processed_frame is not None:\n",
        "            frames.append(processed_frame)\n",
        "\n",
        "    cap.release()\n",
        "    frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "    # Return data\n",
        "    if self.train:\n",
        "        label = 0.0\n",
        "        if metadata['label'] == 'fake':\n",
        "            label = 1.0\n",
        "        return frames, torch.FloatTensor([label]).to(self.device)\n",
        "    else:\n",
        "        return frames\n",
        "\n",
        "def __len__(self):\n",
        "    return len(self.videos)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "lFuk7cI-at9I",
        "outputId": "a82c318a-0391-452e-8562-6ba1a8ee1f1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def __process_frame(self, frame):\\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n    boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\\n\\n    if boxes is None:\\n        # Повертаємо None, якщо обличчя не знайдено\\n        return None\\n    else:\\n        # Беремо перше обличчя (якщо знайдено кілька)\\n        box = boxes[0]\\n\\n        # Розширюємо рамку на 30%\\n        width = box[2] - box[0]\\n        height = box[3] - box[1]\\n        expand_x = width * 0.3 / 2\\n        expand_y = height * 0.3 / 2\\n        x1 = max(int(box[0] - expand_x), 0)\\n        y1 = max(int(box[1] - expand_y), 0)\\n        x2 = min(int(box[2] + expand_x), frame.shape[1])\\n        y2 = min(int(box[3] + expand_y), frame.shape[0])\\n\\n        # Обрізаємо зображення за новими координатами\\n        face = frame[y1:y2, x1:x2]\\n        face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\\n\\n        # Перетворюємо на тензор і нормалізуємо\\n        face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\\n\\n        return face\\n\\ndef __getitem__(self, n):\\n    if self.train:\\n        video, metadata = self.videos[n]\\n    else:\\n        video = self.videos[n]\\n\\n    # Processing video frames\\n    if os.path.islink(video):\\n        video = os.readlink(video)\\n\\n    cap = cv2.VideoCapture(video)\\n\\n    frames = []\\n    while len(frames) < self.n_frames:\\n        ret, frame = cap.read()\\n        if not ret:\\n            # Якщо дійшли до кінця відео, повертаємось на початок\\n            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\\n            continue\\n\\n        processed_frame = self.__process_frame(frame)\\n        if processed_frame is not None:\\n            frames.append(processed_frame)\\n\\n    cap.release()\\n    frames = torch.stack(frames).to(self.device)\\n\\n    # Return data\\n    if self.train:\\n        label = 0.0\\n        if metadata['label'] == 'fake':\\n            label = 1.0\\n        return frames, torch.FloatTensor([label]).to(self.device)\\n    else:\\n        return frames\\n\\ndef __len__(self):\\n    return len(self.videos)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import os.path\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "import glob\n",
        "import datetime\n",
        "import subprocess\n",
        "from facenet_pytorch import MTCNN\n",
        "import shutil\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "class FaceClassifier(nn.Module):\n",
        "    def __init__(self, n_linear_hidden=256, lstm_hidden_dim=128, num_lstm_layers=1, dropout=0.1):\n",
        "        super(FaceClassifier, self).__init__()\n",
        "\n",
        "        # Завантаження попередньо натренованої EfficientNet\n",
        "        self.cnn = models.efficientnet_b7(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.cnn.children())[:-1])  # Виключаємо шар класифікації\n",
        "\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Розмір виходу від feature extractor\n",
        "        self.feature_output_size = 2560  # EfficientNet B7 дає 2560 ознак\n",
        "\n",
        "        # LSTM для обробки послідовності ознак кожного кадру\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_output_size,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        # Повнозв'язні шари для класифікації\n",
        "        self.fc1 = nn.Linear(2 * lstm_hidden_dim, n_linear_hidden)  # множимо на 2 через bidirectional LSTM\n",
        "        self.fc2 = nn.Linear(n_linear_hidden, 1)\n",
        "\n",
        "    def forward(self, vid_frames):\n",
        "        # Витягання ознак для кожного кадру\n",
        "        batch_size, num_frames, channels, height, width = vid_frames.shape\n",
        "        vid_frames = vid_frames.view(batch_size * num_frames, channels, height, width)\n",
        "\n",
        "        # Використовуємо фічерний екстрактор\n",
        "        with torch.no_grad():\n",
        "            vid_features = self.feature_extractor(vid_frames)\n",
        "\n",
        "        # Переформатовуємо ознаки для LSTM\n",
        "        vid_features = vid_features.view(batch_size, num_frames, -1)  # (batch_size, num_frames, feature_output_size)\n",
        "\n",
        "        # Обробка послідовності кадрів через LSTM\n",
        "        lstm_out, _ = self.lstm(vid_features)  # lstm_out: (batch_size, num_frames, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Використання середнього значення по кадрам для об'єднання послідовності (можна також використовувати останній кадр або інші методи агрегації)\n",
        "        lstm_out = torch.mean(lstm_out, dim=1)  # (batch_size, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Класифікаційні шари\n",
        "        x = torch.relu(self.fc1(lstm_out))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0fYr1PNhWIDq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 epocs"
      ],
      "metadata": {
        "id": "RVtVhb_-bolZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fHI7eavJWL31",
        "outputId": "b5283532-fdc3-4a7c-fe32-4a65e1c2d1c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "all test folders: ['/content/fake_videos/train'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 17:02:23.846056\n",
            "using device: cuda\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "82it [06:29,  4.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0, val loss: 0.0, executed in: 389.0905051231384\n",
            "valid total: 82, valid correct: 41.0, valid incorrect: 41.0, valid accuracy: 0.4999999701976776\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.50      0.80      0.62        41\n",
            "        Fake       0.50      0.20      0.28        41\n",
            "\n",
            "    accuracy                           0.50        82\n",
            "   macro avg       0.50      0.50      0.45        82\n",
            "weighted avg       0.50      0.50      0.45        82\n",
            "\n",
            "end time: 2024-11-06 17:08:53.346494\n",
            "executed in: 0:06:29.500438\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    2.2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8 epocs"
      ],
      "metadata": {
        "id": "akg70T-Nbl35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/2_classifier_30_small.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "alvRcVC9bcf8",
        "outputId": "29142d77-bc78-4cd7-9529-bad9951a4634"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "all test folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "start time: 2024-11-06 17:09:51.090149\n",
            "using device: cuda\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "82it [06:28,  4.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0, val loss: 0.0, executed in: 388.4746015071869\n",
            "valid total: 82, valid correct: 46.0, valid incorrect: 36.0, valid accuracy: 0.5609756112098694\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.54      0.83      0.65        41\n",
            "        Fake       0.63      0.29      0.40        41\n",
            "\n",
            "    accuracy                           0.56        82\n",
            "   macro avg       0.59      0.56      0.53        82\n",
            "weighted avg       0.59      0.56      0.53        82\n",
            "\n",
            "end time: 2024-11-06 17:16:19.844527\n",
            "executed in: 0:06:28.754378\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    2.2 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}