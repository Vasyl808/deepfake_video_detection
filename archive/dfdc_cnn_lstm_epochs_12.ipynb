{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasyl808/deepfake_video_detection/blob/main/dfdc_cnn_lstm_epochs_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjCPksT8rTh4",
        "outputId": "75512d13-21df-4de7-d27f-5240e2bcb343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbPVcJOXZOD3",
        "outputId": "028a1428-886b-4709-9df7-a922d09d3d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet_pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: facenet_pytorch\n",
            "Successfully installed facenet_pytorch-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install facenet_pytorch --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_eOd9pvZVob",
        "outputId": "44e14669-de4a-4a4d-aea6-a0c2076485df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Data/deepfake-detection-challenge.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_videos/aassnaulhq.mp4  \n",
            "  inflating: test_videos/aayfryxljh.mp4  \n",
            "  inflating: test_videos/acazlolrpz.mp4  \n",
            "  inflating: test_videos/adohdulfwb.mp4  \n",
            "  inflating: test_videos/ahjnxtiamx.mp4  \n",
            "  inflating: test_videos/ajiyrjfyzp.mp4  \n",
            "  inflating: test_videos/aktnlyqpah.mp4  \n",
            "  inflating: test_videos/alrtntfxtd.mp4  \n",
            "  inflating: test_videos/aomqqjipcp.mp4  \n",
            "  inflating: test_videos/apedduehoy.mp4  \n",
            "  inflating: test_videos/apvzjkvnwn.mp4  \n",
            "  inflating: test_videos/aqrsylrzgi.mp4  \n",
            "  inflating: test_videos/axfhbpkdlc.mp4  \n",
            "  inflating: test_videos/ayipraspbn.mp4  \n",
            "  inflating: test_videos/bcbqxhziqz.mp4  \n",
            "  inflating: test_videos/bcvheslzrq.mp4  \n",
            "  inflating: test_videos/bdshuoldwx.mp4  \n",
            "  inflating: test_videos/bfdopzvxbi.mp4  \n",
            "  inflating: test_videos/bfjsthfhbd.mp4  \n",
            "  inflating: test_videos/bjyaxvggle.mp4  \n",
            "  inflating: test_videos/bkcyglmfci.mp4  \n",
            "  inflating: test_videos/bktkwbcawi.mp4  \n",
            "  inflating: test_videos/bkuzquigyt.mp4  \n",
            "  inflating: test_videos/blnmxntbey.mp4  \n",
            "  inflating: test_videos/blszgmxkvu.mp4  \n",
            "  inflating: test_videos/bnuwxhfahw.mp4  \n",
            "  inflating: test_videos/bofrwgeyjo.mp4  \n",
            "  inflating: test_videos/btdxnajogv.mp4  \n",
            "  inflating: test_videos/bvpeerislp.mp4  \n",
            "  inflating: test_videos/bwdmzwhdnw.mp4  \n",
            "  inflating: test_videos/bzvzpwrabw.mp4  \n",
            "  inflating: test_videos/cekarydqba.mp4  \n",
            "  inflating: test_videos/cekwtyxdoo.mp4  \n",
            "  inflating: test_videos/cjkctqqakb.mp4  \n",
            "  inflating: test_videos/cnpanmywno.mp4  \n",
            "  inflating: test_videos/cnxccbjlct.mp4  \n",
            "  inflating: test_videos/coqwgzpbhx.mp4  \n",
            "  inflating: test_videos/cosghhimnd.mp4  \n",
            "  inflating: test_videos/coujjnypba.mp4  \n",
            "  inflating: test_videos/cqhwesrciw.mp4  \n",
            "  inflating: test_videos/cqxxumarvp.mp4  \n",
            "  inflating: test_videos/csnkohqxdv.mp4  \n",
            "  inflating: test_videos/cxsvvnxpyz.mp4  \n",
            "  inflating: test_videos/czfqlbcfpa.mp4  \n",
            "  inflating: test_videos/dcqodpzomd.mp4  \n",
            "  inflating: test_videos/ddtbarpcgo.mp4  \n",
            "  inflating: test_videos/demuhxssgl.mp4  \n",
            "  inflating: test_videos/didzujjhtg.mp4  \n",
            "  inflating: test_videos/dkuqbduxev.mp4  \n",
            "  inflating: test_videos/dmmvuaikkv.mp4  \n",
            "  inflating: test_videos/dnmowthjcj.mp4  \n",
            "  inflating: test_videos/doniqevxeg.mp4  \n",
            "  inflating: test_videos/dozjwhnedd.mp4  \n",
            "  inflating: test_videos/dpevefkefv.mp4  \n",
            "  inflating: test_videos/dpmgoiwhuf.mp4  \n",
            "  inflating: test_videos/dsnxgrfdmd.mp4  \n",
            "  inflating: test_videos/dtozwcapoa.mp4  \n",
            "  inflating: test_videos/dvkdfhrpph.mp4  \n",
            "  inflating: test_videos/dvtpwatuja.mp4  \n",
            "  inflating: test_videos/dvwpvqdflx.mp4  \n",
            "  inflating: test_videos/dxfdovivlw.mp4  \n",
            "  inflating: test_videos/dxgnpnowgk.mp4  \n",
            "  inflating: test_videos/dyjklprkoc.mp4  \n",
            "  inflating: test_videos/dzkyxbbqkr.mp4  \n",
            "  inflating: test_videos/dzojiwfvba.mp4  \n",
            "  inflating: test_videos/ecumyiowzs.mp4  \n",
            "  inflating: test_videos/eisofhptvk.mp4  \n",
            "  inflating: test_videos/ekboxwrwuv.mp4  \n",
            "  inflating: test_videos/ekelfsnqof.mp4  \n",
            "  inflating: test_videos/ekvwecwltj.mp4  \n",
            "  inflating: test_videos/elackxuccp.mp4  \n",
            "  inflating: test_videos/eppyqpgewp.mp4  \n",
            "  inflating: test_videos/eqslzbqfea.mp4  \n",
            "  inflating: test_videos/eryjktdexi.mp4  \n",
            "  inflating: test_videos/esjdyghhog.mp4  \n",
            "  inflating: test_videos/esmqxszybs.mp4  \n",
            "  inflating: test_videos/espkiocpxq.mp4  \n",
            "  inflating: test_videos/etdliwticv.mp4  \n",
            "  inflating: test_videos/evysmtpnrf.mp4  \n",
            "  inflating: test_videos/eyguqfmgzh.mp4  \n",
            "  inflating: test_videos/eywdmustbb.mp4  \n",
            "  inflating: test_videos/famlupsgqm.mp4  \n",
            "  inflating: test_videos/fddmkqjwsh.mp4  \n",
            "  inflating: test_videos/fjrueenjyp.mp4  \n",
            "  inflating: test_videos/fjxovgmwnm.mp4  \n",
            "  inflating: test_videos/fmhiujydwo.mp4  \n",
            "  inflating: test_videos/fmvvmcbdrw.mp4  \n",
            "  inflating: test_videos/fneqiqpqvs.mp4  \n",
            "  inflating: test_videos/fnxgqcvlsd.mp4  \n",
            "  inflating: test_videos/fopjiyxiqd.mp4  \n",
            "  inflating: test_videos/fpevfidstw.mp4  \n",
            "  inflating: test_videos/fqgypsunzr.mp4  \n",
            "  inflating: test_videos/frqfsucgao.mp4  \n",
            "  inflating: test_videos/fsdrwikhge.mp4  \n",
            "  inflating: test_videos/fwykevubzy.mp4  \n",
            "  inflating: test_videos/fxuxxtryjn.mp4  \n",
            "  inflating: test_videos/fzvpbrzssi.mp4  \n",
            "  inflating: test_videos/gahgyuwzbu.mp4  \n",
            "  inflating: test_videos/gbnzicjyhz.mp4  \n",
            "  inflating: test_videos/gccnvdoknm.mp4  \n",
            "  inflating: test_videos/gcdtglsoqj.mp4  \n",
            "  inflating: test_videos/gfcycflhbo.mp4  \n",
            "  inflating: test_videos/gfdjzwnpyp.mp4  \n",
            "  inflating: test_videos/gfgcwxkbjd.mp4  \n",
            "  inflating: test_videos/ggdpclfcgk.mp4  \n",
            "  inflating: test_videos/ggzjfrirjh.mp4  \n",
            "  inflating: test_videos/ghnpsltzyn.mp4  \n",
            "  inflating: test_videos/gkutjglghz.mp4  \n",
            "  inflating: test_videos/gochxzemmq.mp4  \n",
            "  inflating: test_videos/gpsxfxrjrr.mp4  \n",
            "  inflating: test_videos/gqnaxievjx.mp4  \n",
            "  inflating: test_videos/gunamloolc.mp4  \n",
            "  inflating: test_videos/halvwiltfs.mp4  \n",
            "  inflating: test_videos/hbufmvbium.mp4  \n",
            "  inflating: test_videos/hcanfkwivl.mp4  \n",
            "  inflating: test_videos/hclsparpth.mp4  \n",
            "  inflating: test_videos/hefisnapds.mp4  \n",
            "  inflating: test_videos/heiyoojifp.mp4  \n",
            "  inflating: test_videos/hevcclcklc.mp4  \n",
            "  inflating: test_videos/hfsvqabzfq.mp4  \n",
            "  inflating: test_videos/hicjuubiau.mp4  \n",
            "  inflating: test_videos/hierggamuo.mp4  \n",
            "  inflating: test_videos/hitfycdavv.mp4  \n",
            "  inflating: test_videos/hnfwagcxdf.mp4  \n",
            "  inflating: test_videos/honxqdilvv.mp4  \n",
            "  inflating: test_videos/hqzwudvhih.mp4  \n",
            "  inflating: test_videos/hsbljbsgxr.mp4  \n",
            "  inflating: test_videos/hsbwhlolsn.mp4  \n",
            "  inflating: test_videos/hszwwswewp.mp4  \n",
            "  inflating: test_videos/htzbnroagi.mp4  \n",
            "  inflating: test_videos/huvlwkxoxm.mp4  \n",
            "  inflating: test_videos/hweshqpfwe.mp4  \n",
            "  inflating: test_videos/hxwtsaydal.mp4  \n",
            "  inflating: test_videos/hyjqolupxn.mp4  \n",
            "  inflating: test_videos/hzoiotcykp.mp4  \n",
            "  inflating: test_videos/hzssdinxec.mp4  \n",
            "  inflating: test_videos/ibxfxggtqh.mp4  \n",
            "  inflating: test_videos/icbsahlivv.mp4  \n",
            "  inflating: test_videos/igpvrfjdzc.mp4  \n",
            "  inflating: test_videos/ihglzxzroo.mp4  \n",
            "  inflating: test_videos/iksxzpqxzi.mp4  \n",
            "  inflating: test_videos/ilqwcbprqa.mp4  \n",
            "  inflating: test_videos/imdmhwkkni.mp4  \n",
            "  inflating: test_videos/iorbtaarte.mp4  \n",
            "  inflating: test_videos/ipkpxvwroe.mp4  \n",
            "  inflating: test_videos/ipvwtgdlre.mp4  \n",
            "  inflating: test_videos/irqzdokcws.mp4  \n",
            "  inflating: test_videos/itfsvvmslp.mp4  \n",
            "  inflating: test_videos/iznnzjvaxc.mp4  \n",
            "  inflating: test_videos/jawgcggquk.mp4  \n",
            "  inflating: test_videos/jhczqfefgw.mp4  \n",
            "  inflating: test_videos/jiavqbrkyk.mp4  \n",
            "  inflating: test_videos/jiswxuqzyz.mp4  \n",
            "  inflating: test_videos/jquevmhdvc.mp4  \n",
            "  inflating: test_videos/jsbpkpxwew.mp4  \n",
            "  inflating: test_videos/jsysgmycsx.mp4  \n",
            "  inflating: test_videos/jyfvaequfg.mp4  \n",
            "  inflating: test_videos/jyoxdvxpza.mp4  \n",
            "  inflating: test_videos/jytrvwlewz.mp4  \n",
            "  inflating: test_videos/jzmzdispyo.mp4  \n",
            "  inflating: test_videos/kcjvhgvhpt.mp4  \n",
            "  inflating: test_videos/keioymnobc.mp4  \n",
            "  inflating: test_videos/kezwvsxxzj.mp4  \n",
            "  inflating: test_videos/khpipxnsvx.mp4  \n",
            "  inflating: test_videos/kmcdjxmnoa.mp4  \n",
            "  inflating: test_videos/kmqkiihrmj.mp4  \n",
            "  inflating: test_videos/knxltsvzyu.mp4  \n",
            "  inflating: test_videos/kowiwvrjht.mp4  \n",
            "  inflating: test_videos/kqlvggiqee.mp4  \n",
            "  inflating: test_videos/kvmpmhdxly.mp4  \n",
            "  inflating: test_videos/kwfdyqofzw.mp4  \n",
            "  inflating: test_videos/lbfqksftuo.mp4  \n",
            "  inflating: test_videos/lbigytrrtr.mp4  \n",
            "  inflating: test_videos/lebzjtusnr.mp4  \n",
            "  inflating: test_videos/lhvjzhjxdp.mp4  \n",
            "  inflating: test_videos/ljauauuyka.mp4  \n",
            "  inflating: test_videos/ljouzjaqqe.mp4  \n",
            "  inflating: test_videos/llplvmcvbl.mp4  \n",
            "  inflating: test_videos/lmdyicksrv.mp4  \n",
            "  inflating: test_videos/lnhkjhyhvw.mp4  \n",
            "  inflating: test_videos/lnjkpdviqb.mp4  \n",
            "  inflating: test_videos/lpgxwdgnio.mp4  \n",
            "  inflating: test_videos/lpkgabskbw.mp4  \n",
            "  inflating: test_videos/lujvyveojc.mp4  \n",
            "  inflating: test_videos/lyoslorecs.mp4  \n",
            "  inflating: test_videos/mdfndlljvt.mp4  \n",
            "  inflating: test_videos/mkmgcxaztt.mp4  \n",
            "  inflating: test_videos/mkzaekkvej.mp4  \n",
            "  inflating: test_videos/mllzkpgatp.mp4  \n",
            "  inflating: test_videos/mnowxangqx.mp4  \n",
            "  inflating: test_videos/mnzabbkpmt.mp4  \n",
            "  inflating: test_videos/mohiqoogpb.mp4  \n",
            "  inflating: test_videos/mszblrdprw.mp4  \n",
            "  inflating: test_videos/mwnibuujwz.mp4  \n",
            "  inflating: test_videos/mwwploizlj.mp4  \n",
            "  inflating: test_videos/mxahsihabr.mp4  \n",
            "  inflating: test_videos/mxlipjhmqk.mp4  \n",
            "  inflating: test_videos/ncmpqwmnzb.mp4  \n",
            "  inflating: test_videos/ncoeewrdlo.mp4  \n",
            "  inflating: test_videos/ndikguxzek.mp4  \n",
            "  inflating: test_videos/nikynwcvuh.mp4  \n",
            "  inflating: test_videos/njzshtfmcw.mp4  \n",
            "  inflating: test_videos/nkhzxomani.mp4  \n",
            "  inflating: test_videos/novarhxpbj.mp4  \n",
            "  inflating: test_videos/nplviymzlg.mp4  \n",
            "  inflating: test_videos/nswtvttxre.mp4  \n",
            "  inflating: test_videos/nthpnwylxo.mp4  \n",
            "  inflating: test_videos/nwvloufjty.mp4  \n",
            "  inflating: test_videos/nwvsbmyndn.mp4  \n",
            "  inflating: test_videos/nxgzmgzkfv.mp4  \n",
            "  inflating: test_videos/nxnmkytwze.mp4  \n",
            "  inflating: test_videos/nxzgekegsp.mp4  \n",
            "  inflating: test_videos/nycmyuzpml.mp4  \n",
            "  inflating: test_videos/nymodlmxni.mp4  \n",
            "  inflating: test_videos/oaguiggjyv.mp4  \n",
            "  inflating: test_videos/ocgdbrgmtq.mp4  \n",
            "  inflating: test_videos/oefukgnvel.mp4  \n",
            "  inflating: test_videos/oelqpetgwj.mp4  \n",
            "  inflating: test_videos/ojsxxkalat.mp4  \n",
            "  inflating: test_videos/okgelildpc.mp4  \n",
            "  inflating: test_videos/omphqltjdd.mp4  \n",
            "  inflating: test_videos/ooafcxxfrs.mp4  \n",
            "  inflating: test_videos/oocincvedt.mp4  \n",
            "  inflating: test_videos/oojxonbgow.mp4  \n",
            "  inflating: test_videos/opvqdabdap.mp4  \n",
            "  inflating: test_videos/orekjthsef.mp4  \n",
            "  inflating: test_videos/orixbcfvdz.mp4  \n",
            "  inflating: test_videos/ouaowjmigq.mp4  \n",
            "  inflating: test_videos/owaogcehvc.mp4  \n",
            "  inflating: test_videos/oyqgwjdwaj.mp4  \n",
            "  inflating: test_videos/oysopgovhu.mp4  \n",
            "  inflating: test_videos/papagllumt.mp4  \n",
            "  inflating: test_videos/pcoxcmtroa.mp4  \n",
            "  inflating: test_videos/pcyswtgick.mp4  \n",
            "  inflating: test_videos/pdswwyyntw.mp4  \n",
            "  inflating: test_videos/pdufsewrec.mp4  \n",
            "  inflating: test_videos/petmyhjclt.mp4  \n",
            "  inflating: test_videos/phjvutxpoi.mp4  \n",
            "  inflating: test_videos/pqdeutauqc.mp4  \n",
            "  inflating: test_videos/pqthmvwonf.mp4  \n",
            "  inflating: test_videos/prhmixykhr.mp4  \n",
            "  inflating: test_videos/prwsfljdjo.mp4  \n",
            "  inflating: test_videos/psesikjaxx.mp4  \n",
            "  inflating: test_videos/ptbfnkajyi.mp4  \n",
            "  inflating: test_videos/ptbnewtvon.mp4  \n",
            "  inflating: test_videos/pxcfrszlgi.mp4  \n",
            "  inflating: test_videos/pxjkzvqomp.mp4  \n",
            "  inflating: test_videos/qarqtkvgby.mp4  \n",
            "  inflating: test_videos/qcbkztamqc.mp4  \n",
            "  inflating: test_videos/qclpbcbgeq.mp4  \n",
            "  inflating: test_videos/qdqdsaiitt.mp4  \n",
            "  inflating: test_videos/qhkzlnzruj.mp4  \n",
            "  inflating: test_videos/qhsehzgxqj.mp4  \n",
            "  inflating: test_videos/qlqhjcshpk.mp4  \n",
            "  inflating: test_videos/qlvsqdroqo.mp4  \n",
            "  inflating: test_videos/qooxnxqqjb.mp4  \n",
            "  inflating: test_videos/qqnlrngaft.mp4  \n",
            "  inflating: test_videos/qsjiypnjwi.mp4  \n",
            "  inflating: test_videos/qswlzfgcgj.mp4  \n",
            "  inflating: test_videos/qxyrtwozyw.mp4  \n",
            "  inflating: test_videos/qyyhuvqmyf.mp4  \n",
            "  inflating: test_videos/rcecrgeotc.mp4  \n",
            "  inflating: test_videos/rcjfxxhcal.mp4  \n",
            "  inflating: test_videos/rerpivllud.mp4  \n",
            "  inflating: test_videos/rfjuhbnlro.mp4  \n",
            "  inflating: test_videos/rfwxcinshk.mp4  \n",
            "  inflating: test_videos/rklawjhbpv.mp4  \n",
            "  inflating: test_videos/rktrpsdlci.mp4  \n",
            "  inflating: test_videos/rmlzgerevr.mp4  \n",
            "  inflating: test_videos/rmufsuogzn.mp4  \n",
            "  inflating: test_videos/rnfcjxynfa.mp4  \n",
            "  inflating: test_videos/rrrfjhugvb.mp4  \n",
            "  inflating: test_videos/rtpbawlmxr.mp4  \n",
            "  inflating: test_videos/ruhtnngrqv.mp4  \n",
            "  inflating: test_videos/rukyxomwcx.mp4  \n",
            "  inflating: test_videos/rvvpazsffd.mp4  \n",
            "  inflating: test_videos/rxdoimqble.mp4  \n",
            "  inflating: test_videos/ryxaqpfubf.mp4  \n",
            "  inflating: test_videos/scbdenmaed.mp4  \n",
            "  inflating: test_videos/scrbqgpvzz.mp4  \n",
            "  inflating: test_videos/sfsayjgzrh.mp4  \n",
            "  inflating: test_videos/shnsajrsow.mp4  \n",
            "  inflating: test_videos/siebfpwuhu.mp4  \n",
            "  inflating: test_videos/sjinmmbipg.mp4  \n",
            "  inflating: test_videos/sjkfxrlxxs.mp4  \n",
            "  inflating: test_videos/sjwywglgym.mp4  \n",
            "  inflating: test_videos/sktpeppbkc.mp4  \n",
            "  inflating: test_videos/sngjsueuhs.mp4  \n",
            "  inflating: test_videos/snlyjbnpgw.mp4  \n",
            "  inflating: test_videos/sodvtfqbpf.mp4  \n",
            "  inflating: test_videos/sqixhnilfm.mp4  \n",
            "  inflating: test_videos/srfefmyjvt.mp4  \n",
            "  inflating: test_videos/sufvvwmbha.mp4  \n",
            "  inflating: test_videos/swsaoktwgi.mp4  \n",
            "  inflating: test_videos/sylnrepacf.mp4  \n",
            "  inflating: test_videos/syuxttuyhm.mp4  \n",
            "  inflating: test_videos/syxobtuucp.mp4  \n",
            "  inflating: test_videos/sznkemeqro.mp4  \n",
            "  inflating: test_videos/tejfudfgpq.mp4  \n",
            "  inflating: test_videos/temeqbmzxu.mp4  \n",
            "  inflating: test_videos/temjefwaas.mp4  \n",
            "  inflating: test_videos/tgawasvbbr.mp4  \n",
            "  inflating: test_videos/tjuihawuqm.mp4  \n",
            "  inflating: test_videos/tjywwgftmv.mp4  \n",
            "  inflating: test_videos/toinozytsp.mp4  \n",
            "  inflating: test_videos/tvhjcfnqtg.mp4  \n",
            "  inflating: test_videos/txmnoyiyte.mp4  \n",
            "  inflating: test_videos/txnmkabufs.mp4  \n",
            "  inflating: test_videos/tyjpjpglgx.mp4  \n",
            "  inflating: test_videos/tynfsthodx.mp4  \n",
            "  inflating: test_videos/ucthmsajay.mp4  \n",
            "  inflating: test_videos/udxqbhgvvx.mp4  \n",
            "  inflating: test_videos/uhakqelqri.mp4  \n",
            "  inflating: test_videos/uhrqlmlclw.mp4  \n",
            "  inflating: test_videos/uoccaiathd.mp4  \n",
            "  inflating: test_videos/upmgtackuf.mp4  \n",
            "  inflating: test_videos/uqvxjfpwdo.mp4  \n",
            "  inflating: test_videos/usqqvxcjmg.mp4  \n",
            "  inflating: test_videos/uubgqnvfdl.mp4  \n",
            "  inflating: test_videos/uvrzaczrbx.mp4  \n",
            "  inflating: test_videos/uxuvkrjhws.mp4  \n",
            "  inflating: test_videos/vajkicalux.mp4  \n",
            "  inflating: test_videos/vbcgoyxsvn.mp4  \n",
            "  inflating: test_videos/vdtsbqidjb.mp4  \n",
            "  inflating: test_videos/vhbbwdflyh.mp4  \n",
            "  inflating: test_videos/viteugozpv.mp4  \n",
            "  inflating: test_videos/vizerpsvbz.mp4  \n",
            "  inflating: test_videos/vjljdfopjg.mp4  \n",
            "  inflating: test_videos/vmxfwxgdei.mp4  \n",
            "  inflating: test_videos/vnlzxqwthl.mp4  \n",
            "  inflating: test_videos/voawxrmqyl.mp4  \n",
            "  inflating: test_videos/vokrpfjpeb.mp4  \n",
            "  inflating: test_videos/vssmlqoiti.mp4  \n",
            "  inflating: test_videos/vtunvalyji.mp4  \n",
            "  inflating: test_videos/vurjckblge.mp4  \n",
            "  inflating: test_videos/vvfszaosiv.mp4  \n",
            "  inflating: test_videos/vwxednhlwz.mp4  \n",
            "  inflating: test_videos/wadvzjhwtw.mp4  \n",
            "  inflating: test_videos/waucvvmtkq.mp4  \n",
            "  inflating: test_videos/wclvkepakb.mp4  \n",
            "  inflating: test_videos/wcqvzujamg.mp4  \n",
            "  inflating: test_videos/wcssbghcpc.mp4  \n",
            "  inflating: test_videos/wcvsqnplsk.mp4  \n",
            "  inflating: test_videos/wfzjxzhdkj.mp4  \n",
            "  inflating: test_videos/wixbuuzygv.mp4  \n",
            "  inflating: test_videos/wjhpisoeaj.mp4  \n",
            "  inflating: test_videos/wmoqzxddkb.mp4  \n",
            "  inflating: test_videos/wndursivcx.mp4  \n",
            "  inflating: test_videos/wnlubukrki.mp4  \n",
            "  inflating: test_videos/wqysrieiqu.mp4  \n",
            "  inflating: test_videos/wvgviwnwob.mp4  \n",
            "  inflating: test_videos/wynotylpnm.mp4  \n",
            "  inflating: test_videos/xcruhaccxc.mp4  \n",
            "  inflating: test_videos/xdezcezszc.mp4  \n",
            "  inflating: test_videos/xhtppuyqdr.mp4  \n",
            "  inflating: test_videos/xitgdpzbxv.mp4  \n",
            "  inflating: test_videos/xjvxtuakyd.mp4  \n",
            "  inflating: test_videos/xljemofssi.mp4  \n",
            "  inflating: test_videos/xmkwsnuzyq.mp4  \n",
            "  inflating: test_videos/xphdfgmfmz.mp4  \n",
            "  inflating: test_videos/xrtvqhdibb.mp4  \n",
            "  inflating: test_videos/xugmhbetrw.mp4  \n",
            "  inflating: test_videos/xxzefxwyku.mp4  \n",
            "  inflating: test_videos/yarpxfqejd.mp4  \n",
            "  inflating: test_videos/yaxgpxhavq.mp4  \n",
            "  inflating: test_videos/ybbrkacebd.mp4  \n",
            "  inflating: test_videos/yhjlnisfel.mp4  \n",
            "  inflating: test_videos/yhylappzid.mp4  \n",
            "  inflating: test_videos/yietrwuncf.mp4  \n",
            "  inflating: test_videos/yiykshcbaz.mp4  \n",
            "  inflating: test_videos/yljecirelf.mp4  \n",
            "  inflating: test_videos/yllztsrwjw.mp4  \n",
            "  inflating: test_videos/ylxwcwhjjd.mp4  \n",
            "  inflating: test_videos/yoyhmxtrys.mp4  \n",
            "  inflating: test_videos/ypbtpunjvm.mp4  \n",
            "  inflating: test_videos/yqhouqakbx.mp4  \n",
            "  inflating: test_videos/yronlutbgm.mp4  \n",
            "  inflating: test_videos/ystdtnetgj.mp4  \n",
            "  inflating: test_videos/ytddugrwph.mp4  \n",
            "  inflating: test_videos/ytopzxrswu.mp4  \n",
            "  inflating: test_videos/ywauoonmlr.mp4  \n",
            "  inflating: test_videos/ywxpquomgt.mp4  \n",
            "  inflating: test_videos/yxadevzohx.mp4  \n",
            "  inflating: test_videos/yxirnfyijn.mp4  \n",
            "  inflating: test_videos/yxvmusxvcz.mp4  \n",
            "  inflating: test_videos/yzuestxcbq.mp4  \n",
            "  inflating: test_videos/zbgssotnjm.mp4  \n",
            "  inflating: test_videos/zcxcmneefk.mp4  \n",
            "  inflating: test_videos/zfobicuigx.mp4  \n",
            "  inflating: test_videos/zfrrixsimm.mp4  \n",
            "  inflating: test_videos/zgbhzkditd.mp4  \n",
            "  inflating: test_videos/zgjosltkie.mp4  \n",
            "  inflating: test_videos/ziipxxchai.mp4  \n",
            "  inflating: test_videos/zmxeiipnqb.mp4  \n",
            "  inflating: test_videos/ztyuiqrhdk.mp4  \n",
            "  inflating: test_videos/ztyvglkcsf.mp4  \n",
            "  inflating: test_videos/zuwwbbusgl.mp4  \n",
            "  inflating: test_videos/zxacihctqp.mp4  \n",
            "  inflating: test_videos/zyufpqvpyu.mp4  \n",
            "  inflating: test_videos/zzmgnglanj.mp4  \n",
            "  inflating: train_sample_videos/aagfhgtpmv.mp4  \n",
            "  inflating: train_sample_videos/aapnvogymq.mp4  \n",
            "  inflating: train_sample_videos/abarnvbtwb.mp4  \n",
            "  inflating: train_sample_videos/abofeumbvv.mp4  \n",
            "  inflating: train_sample_videos/abqwwspghj.mp4  \n",
            "  inflating: train_sample_videos/acifjvzvpm.mp4  \n",
            "  inflating: train_sample_videos/acqfdwsrhi.mp4  \n",
            "  inflating: train_sample_videos/acxnxvbsxk.mp4  \n",
            "  inflating: train_sample_videos/acxwigylke.mp4  \n",
            "  inflating: train_sample_videos/aczrgyricp.mp4  \n",
            "  inflating: train_sample_videos/adhsbajydo.mp4  \n",
            "  inflating: train_sample_videos/adohikbdaz.mp4  \n",
            "  inflating: train_sample_videos/adylbeequz.mp4  \n",
            "  inflating: train_sample_videos/aelfnikyqj.mp4  \n",
            "  inflating: train_sample_videos/aelzhcnwgf.mp4  \n",
            "  inflating: train_sample_videos/aettqgevhz.mp4  \n",
            "  inflating: train_sample_videos/aevrfsexku.mp4  \n",
            "  inflating: train_sample_videos/afoovlsmtx.mp4  \n",
            "  inflating: train_sample_videos/agdkmztvby.mp4  \n",
            "  inflating: train_sample_videos/agqphdxmwt.mp4  \n",
            "  inflating: train_sample_videos/agrmhtjdlk.mp4  \n",
            "  inflating: train_sample_videos/ahbweevwpv.mp4  \n",
            "  inflating: train_sample_videos/ahdbuwqxit.mp4  \n",
            "  inflating: train_sample_videos/ahfazfbntc.mp4  \n",
            "  inflating: train_sample_videos/ahqqqilsxt.mp4  \n",
            "  inflating: train_sample_videos/aipfdnwpoo.mp4  \n",
            "  inflating: train_sample_videos/ajqslcypsw.mp4  \n",
            "  inflating: train_sample_videos/ajwpjhrbcv.mp4  \n",
            "  inflating: train_sample_videos/aklqzsddfl.mp4  \n",
            "  inflating: train_sample_videos/aknbdpmgua.mp4  \n",
            "  inflating: train_sample_videos/aknmpoonls.mp4  \n",
            "  inflating: train_sample_videos/akvmwkdyuv.mp4  \n",
            "  inflating: train_sample_videos/akxoopqjqz.mp4  \n",
            "  inflating: train_sample_videos/akzbnazxtz.mp4  \n",
            "  inflating: train_sample_videos/aladcziidp.mp4  \n",
            "  inflating: train_sample_videos/alaijyygdv.mp4  \n",
            "  inflating: train_sample_videos/alninxcyhg.mp4  \n",
            "  inflating: train_sample_videos/altziddtxi.mp4  \n",
            "  inflating: train_sample_videos/alvgwypubw.mp4  \n",
            "  inflating: train_sample_videos/amaivqofda.mp4  \n",
            "  inflating: train_sample_videos/amowujxmzc.mp4  \n",
            "  inflating: train_sample_videos/andaxzscny.mp4  \n",
            "  inflating: train_sample_videos/aneclqfpbt.mp4  \n",
            "  inflating: train_sample_videos/anpuvshzoo.mp4  \n",
            "  inflating: train_sample_videos/aorjvbyxhw.mp4  \n",
            "  inflating: train_sample_videos/apatcsqejh.mp4  \n",
            "  inflating: train_sample_videos/apgjqzkoma.mp4  \n",
            "  inflating: train_sample_videos/apogckdfrz.mp4  \n",
            "  inflating: train_sample_videos/aqpnvjhuzw.mp4  \n",
            "  inflating: train_sample_videos/arkroixhey.mp4  \n",
            "  inflating: train_sample_videos/arlmiizoob.mp4  \n",
            "  inflating: train_sample_videos/arrhsnjqku.mp4  \n",
            "  inflating: train_sample_videos/asaxgevnnp.mp4  \n",
            "  inflating: train_sample_videos/asdpeebotb.mp4  \n",
            "  inflating: train_sample_videos/aslsvlvpth.mp4  \n",
            "  inflating: train_sample_videos/asmpfjfzif.mp4  \n",
            "  inflating: train_sample_videos/asvcrfdpnq.mp4  \n",
            "  inflating: train_sample_videos/atkdltyyen.mp4  \n",
            "  inflating: train_sample_videos/atvmxvwyns.mp4  \n",
            "  inflating: train_sample_videos/atxvxouljq.mp4  \n",
            "  inflating: train_sample_videos/atyntldecu.mp4  \n",
            "  inflating: train_sample_videos/atzdznmder.mp4  \n",
            "  inflating: train_sample_videos/aufmsmnoye.mp4  \n",
            "  inflating: train_sample_videos/augtsuxpzc.mp4  \n",
            "  inflating: train_sample_videos/avfitoutyn.mp4  \n",
            "  inflating: train_sample_videos/avgiuextiz.mp4  \n",
            "  inflating: train_sample_videos/avibnnhwhp.mp4  \n",
            "  inflating: train_sample_videos/avmjormvsx.mp4  \n",
            "  inflating: train_sample_videos/avnqydkqjj.mp4  \n",
            "  inflating: train_sample_videos/avssvvsdhz.mp4  \n",
            "  inflating: train_sample_videos/avtycwsgyb.mp4  \n",
            "  inflating: train_sample_videos/avvdgsennp.mp4  \n",
            "  inflating: train_sample_videos/avywawptfc.mp4  \n",
            "  inflating: train_sample_videos/awhmfnnjih.mp4  \n",
            "  inflating: train_sample_videos/awnwkrqibf.mp4  \n",
            "  inflating: train_sample_videos/awukslzjra.mp4  \n",
            "  inflating: train_sample_videos/axczxisdtb.mp4  \n",
            "  inflating: train_sample_videos/axntxmycwd.mp4  \n",
            "  inflating: train_sample_videos/axoygtekut.mp4  \n",
            "  inflating: train_sample_videos/axwgcsyphv.mp4  \n",
            "  inflating: train_sample_videos/axwovszumc.mp4  \n",
            "  inflating: train_sample_videos/aybgughjxh.mp4  \n",
            "  inflating: train_sample_videos/aybumesmpk.mp4  \n",
            "  inflating: train_sample_videos/ayqvfdhslr.mp4  \n",
            "  inflating: train_sample_videos/aytzyidmgs.mp4  \n",
            "  inflating: train_sample_videos/azpuxunqyo.mp4  \n",
            "  inflating: train_sample_videos/azsmewqghg.mp4  \n",
            "  inflating: train_sample_videos/bahdpoesir.mp4  \n",
            "  inflating: train_sample_videos/bbhpvrmbse.mp4  \n",
            "  inflating: train_sample_videos/bbhtdfuqxq.mp4  \n",
            "  inflating: train_sample_videos/bbvgxeczei.mp4  \n",
            "  inflating: train_sample_videos/bchnbulevv.mp4  \n",
            "  inflating: train_sample_videos/bctvsmddgq.mp4  \n",
            "  inflating: train_sample_videos/bdbhekrrwo.mp4  \n",
            "  inflating: train_sample_videos/bddjdhzfze.mp4  \n",
            "  inflating: train_sample_videos/bdgipnyobr.mp4  \n",
            "  inflating: train_sample_videos/bdnaqemxmr.mp4  \n",
            "  inflating: train_sample_videos/bdxuhamuqx.mp4  \n",
            "  inflating: train_sample_videos/beboztfcme.mp4  \n",
            "  inflating: train_sample_videos/bejhvclboh.mp4  \n",
            "  inflating: train_sample_videos/benmsfzfaz.mp4  \n",
            "  inflating: train_sample_videos/beyebyhrph.mp4  \n",
            "  inflating: train_sample_videos/bffwsjxghk.mp4  \n",
            "  inflating: train_sample_videos/bgaogsjehq.mp4  \n",
            "  inflating: train_sample_videos/bggsurpgpr.mp4  \n",
            "  inflating: train_sample_videos/bghphrsfxf.mp4  \n",
            "  inflating: train_sample_videos/bgmlwsoamc.mp4  \n",
            "  inflating: train_sample_videos/bguwlyazau.mp4  \n",
            "  inflating: train_sample_videos/bgvhtpzknn.mp4  \n",
            "  inflating: train_sample_videos/bgwmmujlmc.mp4  \n",
            "  inflating: train_sample_videos/bhaaboftbc.mp4  \n",
            "  inflating: train_sample_videos/bhbdugnurr.mp4  \n",
            "  inflating: train_sample_videos/bhpwpydzpo.mp4  \n",
            "  inflating: train_sample_videos/bhsluedavd.mp4  \n",
            "  inflating: train_sample_videos/bilnggbxgu.mp4  \n",
            "  inflating: train_sample_videos/bjjbwsqjir.mp4  \n",
            "  inflating: train_sample_videos/bjkmjilrxp.mp4  \n",
            "  inflating: train_sample_videos/bjsmaqefoi.mp4  \n",
            "  inflating: train_sample_videos/bkmdzhfzfh.mp4  \n",
            "  inflating: train_sample_videos/bkvetcojbt.mp4  \n",
            "  inflating: train_sample_videos/bkwxhglwct.mp4  \n",
            "  inflating: train_sample_videos/blpchvmhxx.mp4  \n",
            "  inflating: train_sample_videos/blzydqdfem.mp4  \n",
            "  inflating: train_sample_videos/bmbbkwmxqj.mp4  \n",
            "  inflating: train_sample_videos/bmehkyanbj.mp4  \n",
            "  inflating: train_sample_videos/bmhvktyiwp.mp4  \n",
            "  inflating: train_sample_videos/bmioepcpsx.mp4  \n",
            "  inflating: train_sample_videos/bmjmjmbglm.mp4  \n",
            "  inflating: train_sample_videos/bmjzrlszhi.mp4  \n",
            "  inflating: train_sample_videos/bnbuonyoje.mp4  \n",
            "  inflating: train_sample_videos/bndybcqhfr.mp4  \n",
            "  inflating: train_sample_videos/bnjcdrfuov.mp4  \n",
            "  inflating: train_sample_videos/bntlodcfeg.mp4  \n",
            "  inflating: train_sample_videos/bofqajtwve.mp4  \n",
            "  inflating: train_sample_videos/boovltmuwi.mp4  \n",
            "  inflating: train_sample_videos/bopqhhalml.mp4  \n",
            "  inflating: train_sample_videos/bourlmzsio.mp4  \n",
            "  inflating: train_sample_videos/bpapbctoao.mp4  \n",
            "  inflating: train_sample_videos/bpwzipqtxf.mp4  \n",
            "  inflating: train_sample_videos/bpxckdzddv.mp4  \n",
            "  inflating: train_sample_videos/bqdjzqhcft.mp4  \n",
            "  inflating: train_sample_videos/bqeiblbxtl.mp4  \n",
            "  inflating: train_sample_videos/bqhtpqmmqp.mp4  \n",
            "  inflating: train_sample_videos/bqkdbcqjvb.mp4  \n",
            "  inflating: train_sample_videos/bqnymlsayl.mp4  \n",
            "  inflating: train_sample_videos/bqqpbzjgup.mp4  \n",
            "  inflating: train_sample_videos/bqtuuwzdtr.mp4  \n",
            "  inflating: train_sample_videos/brhalypwoo.mp4  \n",
            "  inflating: train_sample_videos/brvqtabyxj.mp4  \n",
            "  inflating: train_sample_videos/brwrlczjvi.mp4  \n",
            "  inflating: train_sample_videos/bseamdrpbj.mp4  \n",
            "  inflating: train_sample_videos/bsfmwclnqy.mp4  \n",
            "  inflating: train_sample_videos/bsqgziaylx.mp4  \n",
            "  inflating: train_sample_videos/btiysiskpf.mp4  \n",
            "  inflating: train_sample_videos/btjlfpzbdu.mp4  \n",
            "  inflating: train_sample_videos/btjwbtsgln.mp4  \n",
            "  inflating: train_sample_videos/btmsngnqhv.mp4  \n",
            "  inflating: train_sample_videos/btohlidmru.mp4  \n",
            "  inflating: train_sample_videos/btugrnoton.mp4  \n",
            "  inflating: train_sample_videos/btunxncpjh.mp4  \n",
            "  inflating: train_sample_videos/btxlttbpkj.mp4  \n",
            "  inflating: train_sample_videos/bulkxhhknf.mp4  \n",
            "  inflating: train_sample_videos/bvgwelbeof.mp4  \n",
            "  inflating: train_sample_videos/bvzjkezkms.mp4  \n",
            "  inflating: train_sample_videos/bweezhfpzp.mp4  \n",
            "  inflating: train_sample_videos/bwhlgysghg.mp4  \n",
            "  inflating: train_sample_videos/bwipwzzxxu.mp4  \n",
            "  inflating: train_sample_videos/bwuwstvsbw.mp4  \n",
            "  inflating: train_sample_videos/bxzakyopjf.mp4  \n",
            "  inflating: train_sample_videos/bydaidkpdp.mp4  \n",
            "  inflating: train_sample_videos/byfenovjnf.mp4  \n",
            "  inflating: train_sample_videos/byijojkdba.mp4  \n",
            "  inflating: train_sample_videos/byofowlkki.mp4  \n",
            "  inflating: train_sample_videos/byqzyxifza.mp4  \n",
            "  inflating: train_sample_videos/byunigvnay.mp4  \n",
            "  inflating: train_sample_videos/byyqectxqa.mp4  \n",
            "  inflating: train_sample_videos/bzmdrafeex.mp4  \n",
            "  inflating: train_sample_videos/bzythlfnhq.mp4  \n",
            "  inflating: train_sample_videos/caifxvsozs.mp4  \n",
            "  inflating: train_sample_videos/caqbrkogkb.mp4  \n",
            "  inflating: train_sample_videos/cbbibzcoih.mp4  \n",
            "  inflating: train_sample_videos/cbltdtxglo.mp4  \n",
            "  inflating: train_sample_videos/ccfoszqabv.mp4  \n",
            "  inflating: train_sample_videos/ccmonzqfrz.mp4  \n",
            "  inflating: train_sample_videos/cdaxixbosp.mp4  \n",
            "  inflating: train_sample_videos/cdbsbdymzd.mp4  \n",
            "  inflating: train_sample_videos/cdphtzqrvp.mp4  \n",
            "  inflating: train_sample_videos/cdyakrxkia.mp4  \n",
            "  inflating: train_sample_videos/cepxysienc.mp4  \n",
            "  inflating: train_sample_videos/cettndmvzl.mp4  \n",
            "  inflating: train_sample_videos/ceymbecxnj.mp4  \n",
            "  inflating: train_sample_videos/cferslmfwh.mp4  \n",
            "  inflating: train_sample_videos/cffffbcywc.mp4  \n",
            "  inflating: train_sample_videos/cfxkpiweqt.mp4  \n",
            "  inflating: train_sample_videos/cfyduhpbps.mp4  \n",
            "  inflating: train_sample_videos/cglxirfaey.mp4  \n",
            "  inflating: train_sample_videos/cgvrgibpfo.mp4  \n",
            "  inflating: train_sample_videos/chtapglbcj.mp4  \n",
            "  inflating: train_sample_videos/chviwxsfhg.mp4  \n",
            "  inflating: train_sample_videos/chzieimrwu.mp4  \n",
            "  inflating: train_sample_videos/ciyoudyhly.mp4  \n",
            "  inflating: train_sample_videos/cizlkenljw.mp4  \n",
            "  inflating: train_sample_videos/ckbdwedgmc.mp4  \n",
            "  inflating: train_sample_videos/ckjaibzfxa.mp4  \n",
            "  inflating: train_sample_videos/ckkuyewywx.mp4  \n",
            "  inflating: train_sample_videos/cknyxaqouy.mp4  \n",
            "  inflating: train_sample_videos/cksanfsjhc.mp4  \n",
            "  inflating: train_sample_videos/clihsshdkq.mp4  \n",
            "  inflating: train_sample_videos/clrycekyst.mp4  \n",
            "  inflating: train_sample_videos/cmbzllswnl.mp4  \n",
            "  inflating: train_sample_videos/cmxcfkrjiv.mp4  \n",
            "  inflating: train_sample_videos/cnilkgvfei.mp4  \n",
            "  inflating: train_sample_videos/coadfnerlk.mp4  \n",
            "  inflating: train_sample_videos/cobjrlugvp.mp4  \n",
            "  inflating: train_sample_videos/covdcysmbi.mp4  \n",
            "  inflating: train_sample_videos/cpjxareypw.mp4  \n",
            "  inflating: train_sample_videos/cppdvdejkc.mp4  \n",
            "  inflating: train_sample_videos/cprhtltsjp.mp4  \n",
            "  inflating: train_sample_videos/cqfugiqupm.mp4  \n",
            "  inflating: train_sample_videos/cqhngvpgyi.mp4  \n",
            "  inflating: train_sample_videos/cqrskwiqng.mp4  \n",
            "  inflating: train_sample_videos/crezycjqyk.mp4  \n",
            "  inflating: train_sample_videos/crktehraph.mp4  \n",
            "  inflating: train_sample_videos/crzfebnfgb.mp4  \n",
            "  inflating: train_sample_videos/cthdnahrkh.mp4  \n",
            "  inflating: train_sample_videos/ctpqeykqdp.mp4  \n",
            "  inflating: train_sample_videos/cttqtsjvgn.mp4  \n",
            "  inflating: train_sample_videos/ctzmavwror.mp4  \n",
            "  inflating: train_sample_videos/curpwogllm.mp4  \n",
            "  inflating: train_sample_videos/cuzrgrbvil.mp4  \n",
            "  inflating: train_sample_videos/cvaksbpssm.mp4  \n",
            "  inflating: train_sample_videos/cwbacdwrzo.mp4  \n",
            "  inflating: train_sample_videos/cwqlvzefpg.mp4  \n",
            "  inflating: train_sample_videos/cwrtyzndpx.mp4  \n",
            "  inflating: train_sample_videos/cwsbspfzck.mp4  \n",
            "  inflating: train_sample_videos/cwwandrkus.mp4  \n",
            "  inflating: train_sample_videos/cxfujlvsuw.mp4  \n",
            "  inflating: train_sample_videos/cxrfacemmq.mp4  \n",
            "  inflating: train_sample_videos/cxttmymlbn.mp4  \n",
            "  inflating: train_sample_videos/cyboodqqyr.mp4  \n",
            "  inflating: train_sample_videos/cycacemkmt.mp4  \n",
            "  inflating: train_sample_videos/cyclgfjdrv.mp4  \n",
            "  inflating: train_sample_videos/cyxlcuyznd.mp4  \n",
            "  inflating: train_sample_videos/czfunozvwp.mp4  \n",
            "  inflating: train_sample_videos/czkdanyadc.mp4  \n",
            "  inflating: train_sample_videos/czmqpxrqoh.mp4  \n",
            "  inflating: train_sample_videos/dafhtipaml.mp4  \n",
            "  inflating: train_sample_videos/dakiztgtnw.mp4  \n",
            "  inflating: train_sample_videos/dakqwktlbi.mp4  \n",
            "  inflating: train_sample_videos/dbhoxkblzx.mp4  \n",
            "  inflating: train_sample_videos/dbhrpizyeq.mp4  \n",
            "  inflating: train_sample_videos/dbnygxtwek.mp4  \n",
            "  inflating: train_sample_videos/dboxtiehng.mp4  \n",
            "  inflating: train_sample_videos/dbtbbhakdv.mp4  \n",
            "  inflating: train_sample_videos/dbzcqmxzaj.mp4  \n",
            "  inflating: train_sample_videos/dbzpcjntve.mp4  \n",
            "  inflating: train_sample_videos/dcamvmuors.mp4  \n",
            "  inflating: train_sample_videos/dcuiiorugd.mp4  \n",
            "  inflating: train_sample_videos/ddepeddixj.mp4  \n",
            "  inflating: train_sample_videos/ddhfabwpuz.mp4  \n",
            "  inflating: train_sample_videos/ddjggcasdw.mp4  \n",
            "  inflating: train_sample_videos/ddpvuimigj.mp4  \n",
            "  inflating: train_sample_videos/ddqccgmtka.mp4  \n",
            "  inflating: train_sample_videos/degpbqvcay.mp4  \n",
            "  inflating: train_sample_videos/deywhkarol.mp4  \n",
            "  inflating: train_sample_videos/deyyistcrd.mp4  \n",
            "  inflating: train_sample_videos/dfbpceeaox.mp4  \n",
            "  inflating: train_sample_videos/dgmevclvzy.mp4  \n",
            "  inflating: train_sample_videos/dgxrqjdomn.mp4  \n",
            "  inflating: train_sample_videos/dgzklxjmix.mp4  \n",
            "  inflating: train_sample_videos/dhcndnuwta.mp4  \n",
            "  inflating: train_sample_videos/dhcselezer.mp4  \n",
            "  inflating: train_sample_videos/dhevettufk.mp4  \n",
            "  inflating: train_sample_videos/dhjmzhrcav.mp4  \n",
            "  inflating: train_sample_videos/dhkwmjxwrn.mp4  \n",
            "  inflating: train_sample_videos/dhoqofwoxa.mp4  \n",
            "  inflating: train_sample_videos/dhxctgyoqj.mp4  \n",
            "  inflating: train_sample_videos/diomeixhrg.mp4  \n",
            "  inflating: train_sample_videos/diopzaywor.mp4  \n",
            "  inflating: train_sample_videos/diqraixiov.mp4  \n",
            "  inflating: train_sample_videos/diuzrpqjli.mp4  \n",
            "  inflating: train_sample_videos/djvtbgwdcc.mp4  \n",
            "  inflating: train_sample_videos/djvutyvaio.mp4  \n",
            "  inflating: train_sample_videos/djxdyjopjd.mp4  \n",
            "  inflating: train_sample_videos/dkdwxmtpuo.mp4  \n",
            "  inflating: train_sample_videos/dkhlttuvmx.mp4  \n",
            "  inflating: train_sample_videos/dkrvorliqc.mp4  \n",
            "  inflating: train_sample_videos/dkuayagnmc.mp4  \n",
            "  inflating: train_sample_videos/dkwjwbwgey.mp4  \n",
            "  inflating: train_sample_videos/dkzvdrzcnr.mp4  \n",
            "  inflating: train_sample_videos/dlpoieqvfb.mp4  \n",
            "  inflating: train_sample_videos/dlrsbscitn.mp4  \n",
            "  inflating: train_sample_videos/dnexlwbcxq.mp4  \n",
            "  inflating: train_sample_videos/dnhvalzvrt.mp4  \n",
            "  inflating: train_sample_videos/dntkzzzcdh.mp4  \n",
            "  inflating: train_sample_videos/dnyvfblxpm.mp4  \n",
            "  inflating: train_sample_videos/doanjploai.mp4  \n",
            "  inflating: train_sample_videos/dofusvhnib.mp4  \n",
            "  inflating: train_sample_videos/dozyddhild.mp4  \n",
            "  inflating: train_sample_videos/dptbnjnkdg.mp4  \n",
            "  inflating: train_sample_videos/dptrzdvwpg.mp4  \n",
            "  inflating: train_sample_videos/dqnyszdong.mp4  \n",
            "  inflating: train_sample_videos/dqppxmoqdl.mp4  \n",
            "  inflating: train_sample_videos/dqqtjcryjv.mp4  \n",
            "  inflating: train_sample_videos/dqswpjoepo.mp4  \n",
            "  inflating: train_sample_videos/dqzreruvje.mp4  \n",
            "  inflating: train_sample_videos/drcyabprvt.mp4  \n",
            "  inflating: train_sample_videos/drgjzlxzxj.mp4  \n",
            "  inflating: train_sample_videos/drsakwyvqv.mp4  \n",
            "  inflating: train_sample_videos/drtbksnpol.mp4  \n",
            "  inflating: train_sample_videos/dsdoseflas.mp4  \n",
            "  inflating: train_sample_videos/dsgpbgsrdm.mp4  \n",
            "  inflating: train_sample_videos/dsjbknkujw.mp4  \n",
            "  inflating: train_sample_videos/dsndhujjjb.mp4  \n",
            "  inflating: train_sample_videos/dtbpmdqvao.mp4  \n",
            "  inflating: train_sample_videos/dtocdfbwca.mp4  \n",
            "  inflating: train_sample_videos/dubiroskqn.mp4  \n",
            "  inflating: train_sample_videos/dulanfulol.mp4  \n",
            "  inflating: train_sample_videos/duvyaxbzvp.mp4  \n",
            "  inflating: train_sample_videos/duycddgtrl.mp4  \n",
            "  inflating: train_sample_videos/duzuusuajr.mp4  \n",
            "  inflating: train_sample_videos/dvakowbgbt.mp4  \n",
            "  inflating: train_sample_videos/dvumqqhoac.mp4  \n",
            "  inflating: train_sample_videos/dwediigjit.mp4  \n",
            "  inflating: train_sample_videos/dxbqjxrhin.mp4  \n",
            "  inflating: train_sample_videos/dxuliowugt.mp4  \n",
            "  inflating: train_sample_videos/dxuplhwvig.mp4  \n",
            "  inflating: train_sample_videos/dzieklokdr.mp4  \n",
            "  inflating: train_sample_videos/dzqwgqewhu.mp4  \n",
            "  inflating: train_sample_videos/dzvyfiarrq.mp4  \n",
            "  inflating: train_sample_videos/dzwkmcwkwl.mp4  \n",
            "  inflating: train_sample_videos/dzyuwjkjui.mp4  \n",
            "  inflating: train_sample_videos/eahlqmfvtj.mp4  \n",
            "  inflating: train_sample_videos/eajlrktemq.mp4  \n",
            "  inflating: train_sample_videos/ebchwmwayp.mp4  \n",
            "  inflating: train_sample_videos/ebebgmtlcu.mp4  \n",
            "  inflating: train_sample_videos/ebeknhudxq.mp4  \n",
            "  inflating: train_sample_videos/ebkzwjgjhq.mp4  \n",
            "  inflating: train_sample_videos/ebywfrmhtd.mp4  \n",
            "  inflating: train_sample_videos/eckvhdusax.mp4  \n",
            "  inflating: train_sample_videos/ecnihjlfyt.mp4  \n",
            "  inflating: train_sample_videos/ecujsjhscd.mp4  \n",
            "  inflating: train_sample_videos/ecuvtoltue.mp4  \n",
            "  inflating: train_sample_videos/ecwaxgutkc.mp4  \n",
            "  inflating: train_sample_videos/eczrseixwq.mp4  \n",
            "  inflating: train_sample_videos/edyncaijwx.mp4  \n",
            "  inflating: train_sample_videos/eebrkicpry.mp4  \n",
            "  inflating: train_sample_videos/eebserckhh.mp4  \n",
            "  inflating: train_sample_videos/eejswgycjc.mp4  \n",
            "  inflating: train_sample_videos/eekozbeafq.mp4  \n",
            "  inflating: train_sample_videos/eepezmygaq.mp4  \n",
            "  inflating: train_sample_videos/eeyhxisdfh.mp4  \n",
            "  inflating: train_sample_videos/efdyrflcpg.mp4  \n",
            "  inflating: train_sample_videos/efwfxwwlbw.mp4  \n",
            "  inflating: train_sample_videos/egbbcxcuqy.mp4  \n",
            "  inflating: train_sample_videos/eggbjzxnmg.mp4  \n",
            "  inflating: train_sample_videos/egghxjjmfg.mp4  \n",
            "  inflating: train_sample_videos/ehbnclaukr.mp4  \n",
            "  inflating: train_sample_videos/ehccixxzoe.mp4  \n",
            "  inflating: train_sample_videos/ehdkmxgtxh.mp4  \n",
            "  inflating: train_sample_videos/ehevsxtecd.mp4  \n",
            "  inflating: train_sample_videos/ehfiekigla.mp4  \n",
            "  inflating: train_sample_videos/ehieahnhte.mp4  \n",
            "  inflating: train_sample_videos/ehtdtkmmli.mp4  \n",
            "  inflating: train_sample_videos/eiriyukqqy.mp4  \n",
            "  inflating: train_sample_videos/eivxffliio.mp4  \n",
            "  inflating: train_sample_videos/eiwopxzjfn.mp4  \n",
            "  inflating: train_sample_videos/eixwxvxbbn.mp4  \n",
            "  inflating: train_sample_videos/ejkqesyvam.mp4  \n",
            "  inflating: train_sample_videos/ekcrtigpab.mp4  \n",
            "  inflating: train_sample_videos/ekhacizpah.mp4  \n",
            "  inflating: train_sample_videos/ekkdjkirzq.mp4  \n",
            "  inflating: train_sample_videos/elginszwtk.mp4  \n",
            "  inflating: train_sample_videos/ellavthztb.mp4  \n",
            "  inflating: train_sample_videos/elvvackpjh.mp4  \n",
            "  inflating: train_sample_videos/emaalmsonj.mp4  \n",
            "  inflating: train_sample_videos/emfbhytfhc.mp4  \n",
            "  inflating: train_sample_videos/emgjphonqb.mp4  \n",
            "  inflating: train_sample_videos/ensyyivobf.mp4  \n",
            "  inflating: train_sample_videos/eoewqcpbgt.mp4  \n",
            "  inflating: train_sample_videos/eprybmbpba.mp4  \n",
            "  inflating: train_sample_videos/epymyyiblu.mp4  \n",
            "  inflating: train_sample_videos/eqjscdagiv.mp4  \n",
            "  inflating: train_sample_videos/eqnoqyfquo.mp4  \n",
            "  inflating: train_sample_videos/eqvuznuwsa.mp4  \n",
            "  inflating: train_sample_videos/erlvuvjsjf.mp4  \n",
            "  inflating: train_sample_videos/erqgqacbqe.mp4  \n",
            "  inflating: train_sample_videos/errocgcham.mp4  \n",
            "  inflating: train_sample_videos/esckbnkkvb.mp4  \n",
            "  inflating: train_sample_videos/esgftaficx.mp4  \n",
            "  inflating: train_sample_videos/esnntzzajv.mp4  \n",
            "  inflating: train_sample_videos/esxrvsgpvb.mp4  \n",
            "  inflating: train_sample_videos/esyhwdfnxs.mp4  \n",
            "  inflating: train_sample_videos/esyrimvzsa.mp4  \n",
            "  inflating: train_sample_videos/etdcqxabww.mp4  \n",
            "  inflating: train_sample_videos/etejaapnxh.mp4  \n",
            "  inflating: train_sample_videos/etmcruaihe.mp4  \n",
            "  inflating: train_sample_videos/etohcvnzbj.mp4  \n",
            "  inflating: train_sample_videos/eudeqjhdfd.mp4  \n",
            "  inflating: train_sample_videos/eukvucdetx.mp4  \n",
            "  inflating: train_sample_videos/metadata.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Data/deepfake-detection-challenge.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9uEFHpRmX_yK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import os.path\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "import glob\n",
        "import datetime\n",
        "import subprocess\n",
        "from scipy.io import wavfile\n",
        "from facenet_pytorch import MTCNN\n",
        "import shutil\n",
        "\n",
        "\n",
        "class FaceDeepfakeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folders, n_frames=None, n_audio_reads=50027, train=True, device=None, cache_folder='/content/cache'):\n",
        "        \"\"\"n_audio_reads controls the length of the audio sequence: 5000 readings/sec.\"\"\"\n",
        "        self.n_frames = n_frames\n",
        "        self.n_audio_reads = n_audio_reads\n",
        "        self.videos = []\n",
        "        self.train = train\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "        self.cache_folder = cache_folder\n",
        "        self.detector = MTCNN(device='cuda', post_process=False)\n",
        "\n",
        "        # Створюємо директорію кешування, якщо вона не існує\n",
        "        if cache_folder and not os.path.exists(cache_folder):\n",
        "            os.makedirs(cache_folder)\n",
        "\n",
        "        for i in range(len(folders)):\n",
        "            if train:\n",
        "                if i == 0:\n",
        "                    with open('/content/train_sample_videos/metadata.json') as f:\n",
        "                        videos = json.load(f)\n",
        "                        videos = [(os.path.join(folders[i], video), metadata) for (video, metadata) in videos.items()]\n",
        "                        self.videos += videos\n",
        "                else:\n",
        "                    with open(os.path.join(\"/content/test.json\")) as f:\n",
        "                        videos = json.load(f)\n",
        "                        videos = [(os.path.join(folders[i], video), metadata) for (video, metadata) in videos.items()]\n",
        "                        self.videos += videos[:246]\n",
        "            else:\n",
        "                self.videos += glob.glob(folders[i] + \"/*.mp4\")\n",
        "\n",
        "    def __process_frame(self, frame, video_id, frame_idx):\n",
        "        cache_path = os.path.join(self.cache_folder, f\"{video_id}_frame{frame_idx}.pt\") if self.cache_folder else None\n",
        "\n",
        "        # Якщо файл з кешем існує, завантажуємо його\n",
        "        if cache_path and os.path.exists(cache_path):\n",
        "            return torch.load(cache_path)\n",
        "\n",
        "        # Інакше обробляємо кадр\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        boxes, _ = self.detector.detect(frame, landmarks=False)\n",
        "\n",
        "        if boxes is None:\n",
        "            return None\n",
        "        else:\n",
        "            box = boxes[0]\n",
        "            width = box[2] - box[0]\n",
        "            height = box[3] - box[1]\n",
        "            expand_x = width * 0.3 / 2\n",
        "            expand_y = height * 0.3 / 2\n",
        "            x1 = max(int(box[0] - expand_x), 0)\n",
        "            y1 = max(int(box[1] - expand_y), 0)\n",
        "            x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "            y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face = cv2.resize(face, (300, 300))\n",
        "            face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            # Зберігаємо оброблений кадр у кеш\n",
        "            if cache_path:\n",
        "                torch.save(face, cache_path)\n",
        "\n",
        "            return face\n",
        "\n",
        "    def __pad_or_trim_frames(self, frames):\n",
        "        if len(frames) == self.n_frames:\n",
        "            return frames\n",
        "        elif len(frames) < self.n_frames:\n",
        "            # Дублюємо кадри, поки не досягнемо необхідної кількості\n",
        "            num_repeats = (self.n_frames) // len(frames) + 1\n",
        "            frames = frames * num_repeats\n",
        "            frames = frames[:self.n_frames]\n",
        "        elif len(frames) > self.n_frames:\n",
        "            # Залишаємо тільки перші 30 кадрів\n",
        "            frames = frames[:self.n_frames]\n",
        "        return frames\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        if self.train:\n",
        "            video, metadata = self.videos[n]\n",
        "        else:\n",
        "            video = self.videos[n]\n",
        "\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        frames = []\n",
        "        frame_idx = 0\n",
        "        while len(frames) < self.n_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            processed_frame = self.__process_frame(frame, video_id, frame_idx)\n",
        "            if processed_frame is not None:\n",
        "                frames.append(processed_frame)\n",
        "            frame_idx += 1\n",
        "\n",
        "        cap.release()\n",
        "        frames = self.__pad_or_trim_frames(frames)\n",
        "        frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "        if self.train:\n",
        "            label = 0.0 if metadata['label'] == 'REAL' else 1.0\n",
        "            return frames, torch.FloatTensor([label]).to(self.device)\n",
        "        else:\n",
        "            return frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QjVO2b1pYoB5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class FaceClassifier(nn.Module):\n",
        "    def __init__(self, n_linear_hidden=256, lstm_hidden_dim=128, num_lstm_layers=1, dropout=0.1):\n",
        "        super(FaceClassifier, self).__init__()\n",
        "\n",
        "        # Завантаження попередньо натренованої EfficientNet\n",
        "        self.cnn = models.efficientnet_b7(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.cnn.children())[:-1])  # Виключаємо шар класифікації\n",
        "\n",
        "        # Розмір виходу від feature extractor\n",
        "        self.feature_output_size = 2560  # EfficientNet B7 дає 2560 ознак\n",
        "\n",
        "        # LSTM для обробки послідовності ознак кожного кадру\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_output_size,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        # Повнозв'язні шари для класифікації\n",
        "        self.fc1 = nn.Linear(2 * lstm_hidden_dim, n_linear_hidden)  # множимо на 2 через bidirectional LSTM\n",
        "        self.fc2 = nn.Linear(n_linear_hidden, 1)\n",
        "\n",
        "    def forward(self, vid_frames):\n",
        "        # Витягання ознак для кожного кадру\n",
        "        batch_size, num_frames, channels, height, width = vid_frames.shape\n",
        "        vid_frames = vid_frames.view(batch_size * num_frames, channels, height, width)\n",
        "\n",
        "        # Використовуємо фічерний екстрактор\n",
        "        with torch.no_grad():\n",
        "            vid_features = self.feature_extractor(vid_frames)\n",
        "\n",
        "        # Переформатовуємо ознаки для LSTM\n",
        "        vid_features = vid_features.view(batch_size, num_frames, -1)  # (batch_size, num_frames, feature_output_size)\n",
        "\n",
        "        # Обробка послідовності кадрів через LSTM\n",
        "        lstm_out, _ = self.lstm(vid_features)  # lstm_out: (batch_size, num_frames, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Використання середнього значення по кадрам для об'єднання послідовності (можна також використовувати останній кадр або інші методи агрегації)\n",
        "        lstm_out = torch.mean(lstm_out, dim=1)  # (batch_size, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Класифікаційні шари\n",
        "        x = torch.relu(self.fc1(lstm_out))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heOrqKqzY29p",
        "outputId": "f0a15144-ef31-4494-9c55-5a27d4914c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/train_sample_videos', '/content/test_videos'], <class 'list'>\n",
            "all test folders: ['/content/train_sample_videos', '/content/test_videos'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-33-1d06439f1409>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 21:33:34.920113\n",
            "using device: cuda\n",
            "81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]<ipython-input-31-d43f27eddadd>:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(cache_path)\n",
            "27it [05:43, 11.22s/it]"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = ['/content/train_sample_videos', '/content/test_videos']\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = ['/content/train_sample_videos', '/content/test_videos']\n",
        "\n",
        "batch_size = int(8)\n",
        "num_epochs = int(1)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "#test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    model.train()\n",
        "    torch.cuda.empty_cache()\n",
        "    for i, batch in tqdm(enumerate(train_loader)):\n",
        "        # if i * batch_size >= epoch_size:\n",
        "        #     break\n",
        "        video_data, labels = batch\n",
        "        video_data = video_data.to(device)\n",
        "        #audio_data = audio_data.to(device)\n",
        "\n",
        "        output = model(video_data)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.round()\n",
        "\n",
        "        n_wrong = (labels - output).abs().sum()\n",
        "        t_count_wrong += n_wrong\n",
        "        t_count += labels.shape[0]\n",
        "\n",
        "        epoch_t_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_labels_all.extend(labels.cpu().detach().numpy())\n",
        "        train_preds_all.extend(output.cpu().detach().numpy())\n",
        "\n",
        "        #print('.', end='', flush=True)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Перетворіть на numpy-масиви\n",
        "    train_labels_all = np.array(train_labels_all).astype(int)\n",
        "    train_preds_all = np.array(train_preds_all).astype(int)\n",
        "\n",
        "    # Обчисліть та виведіть матрицю плутанини для тренувального набору\n",
        "    conf_matrix_train = confusion_matrix(train_labels_all, train_preds_all)\n",
        "    print(conf_matrix_train)\n",
        "    # plt.figure(figsize=(8, 6))\n",
        "    #sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    #plt.xlabel('Predicted Label')\n",
        "    #plt.ylabel('True Label')\n",
        "    #plt.title('Confusion Matrix (Train Set)')\n",
        "    #plt.show()\n",
        "\n",
        "    # Виведіть класифікаційний звіт для тренувального набору\n",
        "    print(\"Classification Report (Train Set):\")\n",
        "    print(classification_report(train_labels_all, train_preds_all, target_names=['Real', 'Fake']))\n",
        "\n",
        "    #all_labels = []\n",
        "    #all_preds = []\n",
        "\n",
        "    #model.eval()\n",
        "    #with torch.no_grad():\n",
        "    #    v_count = 0\n",
        "    #    v_count_wrong = 0\n",
        "    #    for i, batch in enumerate(val_loader):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "   #         video_data, labels = batch\n",
        "   #         video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "  #          output = model(video_data)\n",
        "  #          loss = criterion(output, labels)\n",
        "\n",
        "            #output = torch.sigmoid(output)\n",
        "            #output = output.round()\n",
        "            #n_wrong = (labels - output).abs().sum()\n",
        "            #v_count_wrong += n_wrong\n",
        "            #v_count += labels.shape[0]\n",
        "\n",
        "            #epoch_v_loss += loss.item()\n",
        "\n",
        "            #all_labels.extend(labels.cpu().numpy())\n",
        "            #all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    train_losses.append(epoch_t_loss/len(train_loader))\n",
        "    #val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    t_count_right = t_count - t_count_wrong\n",
        "    #v_count_right = v_count - v_count_wrong\n",
        "    t_accuracy = t_count_right / t_count\n",
        "    #v_accuracy = v_count_right / v_count\n",
        "\n",
        "    train_accuracies.append(t_accuracy)\n",
        "    #val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, train loss: {train_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    #print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    #all_labels = np.array(all_labels).astype(int)\n",
        "    #all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    #conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    #plt.figure(figsize=(8, 6))\n",
        "    #sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    #plt.xlabel('Predicted Label')\n",
        "    #plt.ylabel('True Label')\n",
        "    #plt.title('Confusion Matrix')\n",
        "    #plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    #print(\"Classification Report:\")\n",
        "    #print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "    #scheduler.step()\n",
        "    ### Saving model per best validation loss\n",
        "    if best_loss > train_losses[-1]:\n",
        "        best_loss = train_losses[-1]\n",
        "        end_time = datetime.datetime.now()\n",
        "        torch.save(model.state_dict(), f'/content/drive/MyDrive/{epoch}_classifier_{n_frames}_small.pt')\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/{epoch}_classifier_{n_frames}_small.pt')\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "#df = pd.DataFrame()\n",
        "#df['train_loss'] = train_losses.cpu().numpy()\n",
        "#df['val_loss'] = val_losses.cpu().numpy()\n",
        "#df['train_acc'] = train_accuracies.cpu().numpy()\n",
        "#df['val_acc'] = val_accuracies.cpu().numpy()\n",
        "#df['epoch_times'] = epoch_times\n",
        "\n",
        "#df.to_csv(f'train_classifier_nframes{n_frames}_bs{batch_size}_lr{lr}.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/14_epochs_classifier_30_small_batch_8_dfdc.pt')"
      ],
      "metadata": {
        "id": "pXx_SQy6a5T6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive_uadfv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vapCtPi7Evg",
        "outputId": "8a36b4f9-a7b0-4d97-b57d-3a7dbf7c7490"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive_uadfv.zip\n",
            "  inflating: fake_videos/test/fake/0046_fake.mp4  \n",
            "  inflating: fake_videos/test/fake/0047_fake.mp4  \n",
            "  inflating: fake_videos/test/fake/0048_fake.mp4  \n",
            "  inflating: fake_videos/test/real/0046.mp4  \n",
            "  inflating: fake_videos/test/real/0047.mp4  \n",
            "  inflating: fake_videos/test/real/0048.mp4  \n",
            "  inflating: fake_videos/train/fake/0000_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0001_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0002_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0003_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0004_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0005_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0006_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0007_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0008_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0009_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0010_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0011_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0012_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0013_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0014_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0015_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0016_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0017_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0018_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0019_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0020_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0021_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0022_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0023_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0024_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0025_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0026_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0027_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0028_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0029_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0030_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0031_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0032_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0033_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0034_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0035_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0036_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0037_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0038_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0039_fake.mp4  \n",
            "  inflating: fake_videos/train/fake/0040_fake.mp4  \n",
            "  inflating: fake_videos/train/real/0000.mp4  \n",
            "  inflating: fake_videos/train/real/0001.mp4  \n",
            "  inflating: fake_videos/train/real/0002.mp4  \n",
            "  inflating: fake_videos/train/real/0003.mp4  \n",
            "  inflating: fake_videos/train/real/0004.mp4  \n",
            "  inflating: fake_videos/train/real/0005.mp4  \n",
            "  inflating: fake_videos/train/real/0006.mp4  \n",
            "  inflating: fake_videos/train/real/0007.mp4  \n",
            "  inflating: fake_videos/train/real/0008.mp4  \n",
            "  inflating: fake_videos/train/real/0009.mp4  \n",
            "  inflating: fake_videos/train/real/0010.mp4  \n",
            "  inflating: fake_videos/train/real/0011.mp4  \n",
            "  inflating: fake_videos/train/real/0012.mp4  \n",
            "  inflating: fake_videos/train/real/0013.mp4  \n",
            "  inflating: fake_videos/train/real/0014.mp4  \n",
            "  inflating: fake_videos/train/real/0015.mp4  \n",
            "  inflating: fake_videos/train/real/0016.mp4  \n",
            "  inflating: fake_videos/train/real/0017.mp4  \n",
            "  inflating: fake_videos/train/real/0018.mp4  \n",
            "  inflating: fake_videos/train/real/0019.mp4  \n",
            "  inflating: fake_videos/train/real/0020.mp4  \n",
            "  inflating: fake_videos/train/real/0021.mp4  \n",
            "  inflating: fake_videos/train/real/0022.mp4  \n",
            "  inflating: fake_videos/train/real/0023.mp4  \n",
            "  inflating: fake_videos/train/real/0024.mp4  \n",
            "  inflating: fake_videos/train/real/0025.mp4  \n",
            "  inflating: fake_videos/train/real/0026.mp4  \n",
            "  inflating: fake_videos/train/real/0027.mp4  \n",
            "  inflating: fake_videos/train/real/0028.mp4  \n",
            "  inflating: fake_videos/train/real/0029.mp4  \n",
            "  inflating: fake_videos/train/real/0030.mp4  \n",
            "  inflating: fake_videos/train/real/0031.mp4  \n",
            "  inflating: fake_videos/train/real/0032.mp4  \n",
            "  inflating: fake_videos/train/real/0033.mp4  \n",
            "  inflating: fake_videos/train/real/0034.mp4  \n",
            "  inflating: fake_videos/train/real/0035.mp4  \n",
            "  inflating: fake_videos/train/real/0036.mp4  \n",
            "  inflating: fake_videos/train/real/0037.mp4  \n",
            "  inflating: fake_videos/train/real/0038.mp4  \n",
            "  inflating: fake_videos/train/real/0040.mp4  \n",
            "  inflating: fake_videos/train/real/0041.mp4  \n",
            "  inflating: fake_videos/validation/fake/0041_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0042_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0043_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0044_fake.mp4  \n",
            "  inflating: fake_videos/validation/fake/0045_fake.mp4  \n",
            "  inflating: fake_videos/validation/real/0039.mp4  \n",
            "  inflating: fake_videos/validation/real/0042.mp4  \n",
            "  inflating: fake_videos/validation/real/0043.mp4  \n",
            "  inflating: fake_videos/validation/real/0044.mp4  \n",
            "  inflating: fake_videos/validation/real/0045.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceDeepfakeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folders, n_frames=None, n_audio_reads=50027, train=True, device=None, cache_folder=None):\n",
        "        \"\"\"n_audio_reads controls the length of the audio sequence: 5000 readings/sec.\"\"\"\n",
        "        self.n_frames = n_frames\n",
        "        self.n_audio_reads = n_audio_reads\n",
        "        self.videos = []\n",
        "        self.train = train\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "        self.cache_folder = cache_folder\n",
        "        self.detector = MTCNN(device='cpu', post_process=False)\n",
        "        for i in range(len(folders)):\n",
        "            if train:\n",
        "                #print(folders[i])\n",
        "                for item in os.listdir(folders[i]):\n",
        "                    item_path = os.path.join(folders[i], item)\n",
        "                    if str(os.path.basename(item_path)) == 'fake':\n",
        "                        for curr in os.listdir(item_path):\n",
        "                            if os.path.isfile(os.path.join(item_path, curr)):\n",
        "                                metadata = {}\n",
        "                                metadata['label']='fake'\n",
        "                                self.videos += [(os.path.join(item_path, curr), metadata)]\n",
        "                    else:\n",
        "                        for item_file in os.listdir(item_path):\n",
        "                            if os.path.isfile(os.path.join(item_path, item_file)):\n",
        "                                metadata = {}\n",
        "                                metadata['label']='real'\n",
        "                                self.videos += [(os.path.join(item_path, item_file), metadata)]\n",
        "            else:\n",
        "                self.videos += glob.glob(folders[i] + \"/*.mp4\")\n",
        "\n",
        "    def __process_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\n",
        "\n",
        "        if boxes is None:\n",
        "            # Повертаємо порожній кадр, якщо обличчя не знайдено\n",
        "            face = torch.zeros((3, 300, 300))\n",
        "        else:\n",
        "            # Беремо перше обличчя (якщо знайдено кілька)\n",
        "            box = boxes[0]\n",
        "\n",
        "            # Розширюємо рамку на 30%\n",
        "            width = box[2] - box[0]\n",
        "            height = box[3] - box[1]\n",
        "            expand_x = width * 0.3 / 2\n",
        "            expand_y = height * 0.3 / 2\n",
        "            x1 = max(int(box[0] - expand_x), 0)\n",
        "            y1 = max(int(box[1] - expand_y), 0)\n",
        "            x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "            y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "            # Обрізаємо зображення за новими координатами\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\n",
        "\n",
        "            # Перетворюємо на тензор і нормалізуємо\n",
        "            face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return face\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        if self.train:\n",
        "            video, metadata = self.videos[n]\n",
        "        else:\n",
        "            video = self.videos[n]\n",
        "\n",
        "        # Processing video frames\n",
        "        if os.path.islink(video):\n",
        "            video = os.readlink(video)\n",
        "\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        frames = []\n",
        "        for _ in range(self.n_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(self.__process_frame(frame))\n",
        "        '''total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step = max(1, total_frames // self.n_frames)\n",
        "\n",
        "        for i in range(self.n_frames):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frames.append(self.__process_frame(frame))\n",
        "            else:\n",
        "                break'''\n",
        "        cap.release()\n",
        "        frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "        # Return data\n",
        "        if self.train:\n",
        "            label = 0.0\n",
        "            if metadata['label'] == 'fake':\n",
        "                label = 1.0\n",
        "            return frames, torch.FloatTensor([label]).to(self.device)\n",
        "        else:\n",
        "            return frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "\n",
        "'''def __process_frame(self, frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\n",
        "\n",
        "    if boxes is None:\n",
        "        # Повертаємо None, якщо обличчя не знайдено\n",
        "        return None\n",
        "    else:\n",
        "        # Беремо перше обличчя (якщо знайдено кілька)\n",
        "        box = boxes[0]\n",
        "\n",
        "        # Розширюємо рамку на 30%\n",
        "        width = box[2] - box[0]\n",
        "        height = box[3] - box[1]\n",
        "        expand_x = width * 0.3 / 2\n",
        "        expand_y = height * 0.3 / 2\n",
        "        x1 = max(int(box[0] - expand_x), 0)\n",
        "        y1 = max(int(box[1] - expand_y), 0)\n",
        "        x2 = min(int(box[2] + expand_x), frame.shape[1])\n",
        "        y2 = min(int(box[3] + expand_y), frame.shape[0])\n",
        "\n",
        "        # Обрізаємо зображення за новими координатами\n",
        "        face = frame[y1:y2, x1:x2]\n",
        "        face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\n",
        "\n",
        "        # Перетворюємо на тензор і нормалізуємо\n",
        "        face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return face\n",
        "\n",
        "def __getitem__(self, n):\n",
        "    if self.train:\n",
        "        video, metadata = self.videos[n]\n",
        "    else:\n",
        "        video = self.videos[n]\n",
        "\n",
        "    # Processing video frames\n",
        "    if os.path.islink(video):\n",
        "        video = os.readlink(video)\n",
        "\n",
        "    cap = cv2.VideoCapture(video)\n",
        "\n",
        "    frames = []\n",
        "    while len(frames) < self.n_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            # Якщо дійшли до кінця відео, повертаємось на початок\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "            continue\n",
        "\n",
        "        processed_frame = self.__process_frame(frame)\n",
        "        if processed_frame is not None:\n",
        "            frames.append(processed_frame)\n",
        "\n",
        "    cap.release()\n",
        "    frames = torch.stack(frames).to(self.device)\n",
        "\n",
        "    # Return data\n",
        "    if self.train:\n",
        "        label = 0.0\n",
        "        if metadata['label'] == 'fake':\n",
        "            label = 1.0\n",
        "        return frames, torch.FloatTensor([label]).to(self.device)\n",
        "    else:\n",
        "        return frames\n",
        "\n",
        "def __len__(self):\n",
        "    return len(self.videos)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "lFuk7cI-at9I",
        "outputId": "31d2dae1-1abf-449d-96b6-aa2230ccc09e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def __process_frame(self, frame):\\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n    boxes, _ = self.detector.detect(frame, landmarks=False)  # Отримуємо координати обличчя\\n\\n    if boxes is None:\\n        # Повертаємо None, якщо обличчя не знайдено\\n        return None\\n    else:\\n        # Беремо перше обличчя (якщо знайдено кілька)\\n        box = boxes[0]\\n\\n        # Розширюємо рамку на 30%\\n        width = box[2] - box[0]\\n        height = box[3] - box[1]\\n        expand_x = width * 0.3 / 2\\n        expand_y = height * 0.3 / 2\\n        x1 = max(int(box[0] - expand_x), 0)\\n        y1 = max(int(box[1] - expand_y), 0)\\n        x2 = min(int(box[2] + expand_x), frame.shape[1])\\n        y2 = min(int(box[3] + expand_y), frame.shape[0])\\n\\n        # Обрізаємо зображення за новими координатами\\n        face = frame[y1:y2, x1:x2]\\n        face = cv2.resize(face, (300, 300))  # Масштабуємо до 300x300\\n\\n        # Перетворюємо на тензор і нормалізуємо\\n        face = torch.from_numpy(face).permute(2, 0, 1).float() / 255.0\\n\\n        return face\\n\\ndef __getitem__(self, n):\\n    if self.train:\\n        video, metadata = self.videos[n]\\n    else:\\n        video = self.videos[n]\\n\\n    # Processing video frames\\n    if os.path.islink(video):\\n        video = os.readlink(video)\\n\\n    cap = cv2.VideoCapture(video)\\n\\n    frames = []\\n    while len(frames) < self.n_frames:\\n        ret, frame = cap.read()\\n        if not ret:\\n            # Якщо дійшли до кінця відео, повертаємось на початок\\n            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\\n            continue\\n\\n        processed_frame = self.__process_frame(frame)\\n        if processed_frame is not None:\\n            frames.append(processed_frame)\\n\\n    cap.release()\\n    frames = torch.stack(frames).to(self.device)\\n\\n    # Return data\\n    if self.train:\\n        label = 0.0\\n        if metadata['label'] == 'fake':\\n            label = 1.0\\n        return frames, torch.FloatTensor([label]).to(self.device)\\n    else:\\n        return frames\\n\\ndef __len__(self):\\n    return len(self.videos)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import os.path\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "import glob\n",
        "import datetime\n",
        "import subprocess\n",
        "from facenet_pytorch import MTCNN\n",
        "import shutil\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "class FaceClassifier(nn.Module):\n",
        "    def __init__(self, n_linear_hidden=256, lstm_hidden_dim=128, num_lstm_layers=1, dropout=0.1):\n",
        "        super(FaceClassifier, self).__init__()\n",
        "\n",
        "        # Завантаження попередньо натренованої EfficientNet\n",
        "        self.cnn = models.efficientnet_b7(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.cnn.children())[:-1])  # Виключаємо шар класифікації\n",
        "\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Розмір виходу від feature extractor\n",
        "        self.feature_output_size = 2560  # EfficientNet B7 дає 2560 ознак\n",
        "\n",
        "        # LSTM для обробки послідовності ознак кожного кадру\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_output_size,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        # Повнозв'язні шари для класифікації\n",
        "        self.fc1 = nn.Linear(2 * lstm_hidden_dim, n_linear_hidden)  # множимо на 2 через bidirectional LSTM\n",
        "        self.fc2 = nn.Linear(n_linear_hidden, 1)\n",
        "\n",
        "    def forward(self, vid_frames):\n",
        "        # Витягання ознак для кожного кадру\n",
        "        batch_size, num_frames, channels, height, width = vid_frames.shape\n",
        "        vid_frames = vid_frames.view(batch_size * num_frames, channels, height, width)\n",
        "\n",
        "        # Використовуємо фічерний екстрактор\n",
        "        with torch.no_grad():\n",
        "            vid_features = self.feature_extractor(vid_frames)\n",
        "\n",
        "        # Переформатовуємо ознаки для LSTM\n",
        "        vid_features = vid_features.view(batch_size, num_frames, -1)  # (batch_size, num_frames, feature_output_size)\n",
        "\n",
        "        # Обробка послідовності кадрів через LSTM\n",
        "        lstm_out, _ = self.lstm(vid_features)  # lstm_out: (batch_size, num_frames, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Використання середнього значення по кадрам для об'єднання послідовності (можна також використовувати останній кадр або інші методи агрегації)\n",
        "        lstm_out = torch.mean(lstm_out, dim=1)  # (batch_size, 2 * lstm_hidden_dim)\n",
        "\n",
        "        # Класифікаційні шари\n",
        "        x = torch.relu(self.fc1(lstm_out))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0fYr1PNhWIDq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 epocs"
      ],
      "metadata": {
        "id": "RVtVhb_-bolZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fHI7eavJWL31",
        "outputId": "c61ac65e-8ac7-433a-b8c7-7fab3a129684"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "all test folders: ['/content/fake_videos/train'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-14-e490b5be23b8>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/10_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 20:31:22.847334\n",
            "using device: cuda\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "82it [06:23,  4.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0, val loss: 0.0, executed in: 383.1442313194275\n",
            "valid total: 82, valid correct: 48.0, valid incorrect: 34.0, valid accuracy: 0.5853658318519592\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.55      0.95      0.70        41\n",
            "        Fake       0.82      0.22      0.35        41\n",
            "\n",
            "    accuracy                           0.59        82\n",
            "   macro avg       0.68      0.59      0.52        82\n",
            "weighted avg       0.68      0.59      0.52        82\n",
            "\n",
            "end time: 2024-11-06 20:37:46.645951\n",
            "executed in: 0:06:23.798617\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.6 GB\n",
            "Cached:    2.4 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e490b5be23b8>:178: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 epocs"
      ],
      "metadata": {
        "id": "akg70T-Nbl35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/11_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "alvRcVC9bcf8",
        "outputId": "97cf80a5-4b2f-48c5-ee70-797e0ebadb97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "all test folders: ['/content/fake_videos/train'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-22-76d58870b7a0>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/11_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 20:57:15.111772\n",
            "using device: cuda\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "82it [06:21,  4.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0, val loss: 0.0, executed in: 381.8708453178406\n",
            "valid total: 82, valid correct: 45.0, valid incorrect: 37.0, valid accuracy: 0.5487804412841797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.53      0.98      0.68        41\n",
            "        Fake       0.83      0.12      0.21        41\n",
            "\n",
            "    accuracy                           0.55        82\n",
            "   macro avg       0.68      0.55      0.45        82\n",
            "weighted avg       0.68      0.55      0.45        82\n",
            "\n",
            "end time: 2024-11-06 21:03:37.255314\n",
            "executed in: 0:06:22.143542\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.8 GB\n",
            "Cached:    2.6 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-76d58870b7a0>:178: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epocs 12"
      ],
      "metadata": {
        "id": "Gmjwyg6iSrNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/12_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eijuINAmSpZj",
        "outputId": "6afaceab-41aa-4f32-e6f9-725cf7ecf59c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all train folders: ['/content/fake_videos/train'], <class 'list'>\n",
            "all test folders: ['/content/fake_videos/train'], <class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-30-dcaada916080>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/12_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n",
            "/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start time: 2024-11-06 21:21:33.260201\n",
            "using device: cuda\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "82it [06:26,  4.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0, val loss: 0.0, executed in: 386.8004381656647\n",
            "valid total: 82, valid correct: 48.0, valid incorrect: 34.0, valid accuracy: 0.5853658318519592\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDOUlEQVR4nO3deVhUdf//8degMiIIiKJAKi6YS+4tRihqblmappVL3oKpZWGZqBWVpaZRdrtVpt1laqZ5p6XdmmXmmrncai5ZaeJyUwpqGiggg8H5/dHX+TXiwijDTHOej65zXc2ZM+e8D9cF19vX53M+YzEMwxAAAABMw8fdBQAAAKBk0QACAACYDA0gAACAydAAAgAAmAwNIAAAgMnQAAIAAJgMDSAAAIDJ0AACAACYDA0gAACAydAAAriiAwcOqGPHjgoKCpLFYtHSpUuL9fxHjhyRxWLRnDlzivW8f2dt2rRRmzZt3F0GAC9GAwj8DRw8eFCPPvqoatWqpbJlyyowMFAxMTGaNm2azp0759Jrx8XF6fvvv9eECRM0b9483XLLLS69XkmKj4+XxWJRYGDgJX+OBw4ckMVikcVi0T//+U+nz3/s2DGNGTNGu3btKoZqAaD4lHZ3AQCu7PPPP9cDDzwgq9Wq/v37q2HDhsrLy9PGjRs1atQo/fDDD/rXv/7lkmufO3dOmzdv1vPPP6+hQ4e65BqRkZE6d+6cypQp45LzX03p0qWVk5OjZcuW6cEHH3R4b/78+Spbtqxyc3Ov6dzHjh3T2LFjVaNGDTVt2rTIn/vqq6+u6XoAUFQ0gIAHO3z4sHr37q3IyEitWbNG4eHh9vcSEhKUkpKizz//3GXXP3nypCQpODjYZdewWCwqW7asy85/NVarVTExMfroo48KNYALFizQPffco08++aREasnJyVG5cuXk6+tbItcDYF4MAQMebOLEicrKytKsWbMcmr8LoqKiNGzYMPvrP/74Qy+//LJq164tq9WqGjVq6LnnnpPNZnP4XI0aNdSlSxdt3LhRt912m8qWLatatWrpgw8+sB8zZswYRUZGSpJGjRoli8WiGjVqSPpz6PTC///VmDFjZLFYHPatWrVKLVu2VHBwsAICAlS3bl0999xz9vcvNwdwzZo1atWqlfz9/RUcHKxu3brpp59+uuT1UlJSFB8fr+DgYAUFBWnAgAHKycm5/A/2In379tUXX3yhjIwM+75t27bpwIED6tu3b6HjT58+rZEjR6pRo0YKCAhQYGCgOnfurN27d9uPWbdunW699VZJ0oABA+xDyRfus02bNmrYsKF27Nih2NhYlStXzv5zuXgOYFxcnMqWLVvo/jt16qQKFSro2LFjRb5XAJBoAAGPtmzZMtWqVUt33HFHkY4fNGiQXnzxRTVv3lxTpkxR69atlZycrN69exc6NiUlRffff786dOigSZMmqUKFCoqPj9cPP/wgSerRo4emTJkiSerTp4/mzZunqVOnOlX/Dz/8oC5dushms2ncuHGaNGmS7r33Xn377bdX/NzXX3+tTp066cSJExozZowSExO1adMmxcTE6MiRI4WOf/DBB3X27FklJyfrwQcf1Jw5czR27Ngi19mjRw9ZLBZ9+umn9n0LFixQvXr11Lx580LHHzp0SEuXLlWXLl00efJkjRo1St9//71at25tb8bq16+vcePGSZIeeeQRzZs3T/PmzVNsbKz9PKdOnVLnzp3VtGlTTZ06VW3btr1kfdOmTVNoaKji4uKUn58vSXrnnXf01Vdf6c0331RERESR7xUAJEkGAI+UmZlpSDK6detWpON37dplSDIGDRrksH/kyJGGJGPNmjX2fZGRkYYkY8OGDfZ9J06cMKxWqzFixAj7vsOHDxuSjNdff93hnHFxcUZkZGShGl566SXjr39WpkyZYkgyTp48edm6L1xj9uzZ9n1NmzY1KleubJw6dcq+b/fu3YaPj4/Rv3//Qtd7+OGHHc553333GRUrVrzsNf96H/7+/oZhGMb9999vtGvXzjAMw8jPzzfCwsKMsWPHXvJnkJuba+Tn5xe6D6vVaowbN86+b9u2bYXu7YLWrVsbkoyZM2de8r3WrVs77Fu5cqUhyRg/frxx6NAhIyAgwOjevftV7xEALoUEEPBQZ86ckSSVL1++SMevWLFCkpSYmOiwf8SIEZJUaK5ggwYN1KpVK/vr0NBQ1a1bV4cOHbrmmi92Ye7gZ599poKCgiJ9Ji0tTbt27VJ8fLxCQkLs+xs3bqwOHTrY7/OvhgwZ4vC6VatWOnXqlP1nWBR9+/bVunXrlJ6erjVr1ig9Pf2Sw7/Sn/MGfXz+/POZn5+vU6dO2Ye3v/vuuyJf02q1asCAAUU6tmPHjnr00Uc1btw49ejRQ2XLltU777xT5GsBwF/RAAIeKjAwUJJ09uzZIh3/v//9Tz4+PoqKinLYHxYWpuDgYP3vf/9z2F+9evVC56hQoYJ+//33a6y4sF69eikmJkaDBg1SlSpV1Lt3b3388cdXbAYv1Fm3bt1C79WvX1+//fabsrOzHfZffC8VKlSQJKfu5e6771b58uX173//W/Pnz9ett95a6Gd5QUFBgaZMmaI6derIarWqUqVKCg0N1Z49e5SZmVnka95www1OPfDxz3/+UyEhIdq1a5feeOMNVa5cucifBYC/ogEEPFRgYKAiIiK0d+9epz538UMYl1OqVKlL7jcM45qvcWF+2gV+fn7asGGDvv76a/3jH//Qnj171KtXL3Xo0KHQsdfjeu7lAqvVqh49emju3LlasmTJZdM/SXrllVeUmJio2NhYffjhh1q5cqVWrVqlm266qchJp/Tnz8cZO3fu1IkTJyRJ33//vVOfBYC/ogEEPFiXLl108OBBbd68+arHRkZGqqCgQAcOHHDYf/z4cWVkZNif6C0OFSpUcHhi9oKLU0ZJ8vHxUbt27TR58mT9+OOPmjBhgtasWaO1a9de8twX6ty/f3+h9/bt26dKlSrJ39//+m7gMvr27audO3fq7Nmzl3xw5oLFixerbdu2mjVrlnr37q2OHTuqffv2hX4mRW3GiyI7O1sDBgxQgwYN9Mgjj2jixInatm1bsZ0fgLnQAAIe7Omnn5a/v78GDRqk48ePF3r/4MGDmjZtmqQ/hzAlFXpSd/LkyZKke+65p9jqql27tjIzM7Vnzx77vrS0NC1ZssThuNOnTxf67IUFkS9emuaC8PBwNW3aVHPnznVoqPbu3auvvvrKfp+u0LZtW7388st66623FBYWdtnjSpUqVShdXLRokY4ePeqw70Kjeqlm2VnPPPOMUlNTNXfuXE2ePFk1atRQXFzcZX+OAHAlLAQNeLDatWtrwYIF6tWrl+rXr+/wTSCbNm3SokWLFB8fL0lq0qSJ4uLi9K9//UsZGRlq3bq1/vvf/2ru3Lnq3r37ZZcYuRa9e/fWM888o/vuu09PPvmkcnJyNGPGDN14440OD0GMGzdOGzZs0D333KPIyEidOHFCb7/9tqpWraqWLVte9vyvv/66OnfurOjoaA0cOFDnzp3Tm2++qaCgII0ZM6bY7uNiPj4+euGFF656XJcuXTRu3DgNGDBAd9xxh77//nvNnz9ftWrVcjiudu3aCg4O1syZM1W+fHn5+/urRYsWqlmzplN1rVmzRm+//bZeeukl+7I0s2fPVps2bTR69GhNnDjRqfMBAMvAAH8DP//8szF48GCjRo0ahq+vr1G+fHkjJibGePPNN43c3Fz7cefPnzfGjh1r1KxZ0yhTpoxRrVo1IykpyeEYw/hzGZh77rmn0HUuXn7kcsvAGIZhfPXVV0bDhg0NX19fo27dusaHH35YaBmY1atXG926dTMiIiIMX19fIyIiwujTp4/x888/F7rGxUulfP3110ZMTIzh5+dnBAYGGl27djV+/PFHh2MuXO/iZWZmz55tSDIOHz582Z+pYTguA3M5l1sGZsSIEUZ4eLjh5+dnxMTEGJs3b77k8i2fffaZ0aBBA6N06dIO99m6dWvjpptuuuQ1/3qeM2fOGJGRkUbz5s2N8+fPOxw3fPhww8fHx9i8efMV7wEALmYxDCdmSQMAAOBvjzmAAAAAJkMDCAAAYDI0gAAAACZDAwgAAGAyNIAAAAAmQwMIAABgMjSAAAAAJuOV3wTi12you0sA4CK/b3vL3SUAcJGybuxKXNk7nNvpeX+3SAABAABMxisTQAAAAKdYzJWJ0QACAABYLO6uoESZq90FAAAACSAAAIDZhoDNdbcAAAAgAQQAAGAOIAAAALwaCSAAAABzAAEAAODNSAABAABMNgeQBhAAAIAhYAAAAHgzEkAAAACTDQGTAAIAAJgMCSAAAABzAAEAAODNSAABAACYAwgAAABvRgIIAABgsjmANIAAAAAMAQMAAMCbkQACAACYbAjYXHcLAAAAEkAAAAASQAAAAHg1EkAAAAAfngIGAACAFyMBBAAAMNkcQBpAAAAAFoIGAACANyMBBAAAMNkQsLnuFgAAACSAAAAAzAEEAACAW8yYMUONGzdWYGCgAgMDFR0drS+++ML+fps2bWSxWBy2IUOGOH0dEkAAAAAPmQNYtWpVvfrqq6pTp44Mw9DcuXPVrVs37dy5UzfddJMkafDgwRo3bpz9M+XKlXP6OjSAAAAAHqJr164OrydMmKAZM2Zoy5Yt9gawXLlyCgsLu67reEa7CwAA4E4Wi8s2m82mM2fOOGw2m+2qJeXn52vhwoXKzs5WdHS0ff/8+fNVqVIlNWzYUElJScrJyXH6dmkAAQAALD4u25KTkxUUFOSwJScnX7aU77//XgEBAbJarRoyZIiWLFmiBg0aSJL69u2rDz/8UGvXrlVSUpLmzZunfv36OX+7hmEY1/zD8lB+zYa6uwQALvL7trfcXQIAFynrxolpfndNdtm5Mz5LKJT4Wa1WWa3WSx6fl5en1NRUZWZmavHixXrvvfe0fv16exP4V2vWrFG7du2UkpKi2rVrF7km5gACAAC4cBmYKzV7l+Lr66uoqChJ0s0336xt27Zp2rRpeueddwod26JFC0lyugFkCBgAAMCDFRQUXHbO4K5duyRJ4eHhTp2TBBAAAMBDloFJSkpS586dVb16dZ09e1YLFizQunXrtHLlSh08eFALFizQ3XffrYoVK2rPnj0aPny4YmNj1bhxY6euQwMIAADgIU6cOKH+/fsrLS1NQUFBaty4sVauXKkOHTrol19+0ddff62pU6cqOztb1apVU8+ePfXCCy84fR0aQAAAAA/5KrhZs2Zd9r1q1app/fr1xXIdz8g7AQAAUGJIAAEAADxkDmBJoQEEAAAwWQNorrsFAAAACSAAAICnPARSUkgAAQAATIYEEAAAgDmAAAAA8GYkgAAAAMwBBAAAgDcjAQQAADDZHEAaQAAAAIaAAQAA4M1IAAEAgOlZSAABAADgzUgAAQCA6ZEAAgAAwKuRAAIAAJgrACQBBAAAMBsSQAAAYHpmmwNIAwgAAEzPbA0gQ8AAAAAmQwIIAABMjwQQAAAAXo0EEAAAmB4JIAAAALwaCSAAAIC5AkASQAAAALMhAQQAAKbHHEAAAAB4NRJAAABgemZLAGkAAQCA6ZmtAWQIGAAAwGRIAAEAgOmRAAIAAMCrkQACAACYKwAkAQQAADAbEkAAAGB6zAEEAACAVyMBBAAApme2BJAGEAAAmJ7ZGkCGgAEAAEyGBBAAAMBcASAJIAAAgNmQAAIAANNjDiAAAAC8GgkgAAAwPRJAAAAAeDUSQAAAYHpmSwBpAAEAgOmZrQFkCBgAAMBkSAABAADMFQCSAAIAAJgNCSAAADA95gACAADAq9EAAgAA07NYLC7bnDFjxgw1btxYgYGBCgwMVHR0tL744gv7+7m5uUpISFDFihUVEBCgnj176vjx407fLw0gAACAh6hatapeffVV7dixQ9u3b9edd96pbt266YcffpAkDR8+XMuWLdOiRYu0fv16HTt2TD169HD6OhbDMIziLt7d/JoNdXcJAFzk921vubsEAC5S1o1PJlRL+Mxl5/5lerfr+nxISIhef/113X///QoNDdWCBQt0//33S5L27dun+vXra/Pmzbr99tuLfE4SQAAAAIvrNpvNpjNnzjhsNpvtqiXl5+dr4cKFys7OVnR0tHbs2KHz58+rffv29mPq1aun6tWra/PmzU7drtt6bWfiyk8//dSFlQAAALhOcnKyxo4d67DvpZde0pgxYy55/Pfff6/o6Gjl5uYqICBAS5YsUYMGDbRr1y75+voqODjY4fgqVaooPT3dqZrc1gAGBQW569IAAAAOXLkMTFJSkhITEx32Wa3Wyx5ft25d7dq1S5mZmVq8eLHi4uK0fv36Yq3JbQ3g7Nmz3XVpAACAEmO1Wq/Y8F3M19dXUVFRkqSbb75Z27Zt07Rp09SrVy/l5eUpIyPDIQU8fvy4wsLCnKqJOYAAAMD0PGUZmEspKCiQzWbTzTffrDJlymj16tX29/bv36/U1FRFR0c7dU6P+SaQxYsX6+OPP1Zqaqry8vIc3vvuu+/cVBUAAEDJSUpKUufOnVW9enWdPXtWCxYs0Lp167Ry5UoFBQVp4MCBSkxMVEhIiAIDA/XEE08oOjraqSeAJQ9pAN944w09//zzio+P12effaYBAwbo4MGD2rZtmxISEtxdHjzA4AdaavD9rRQZESJJ+ulQul751xf66tsfJUlVKpbXK0/dpztvr6fy/lb9fOSEJs5aqaWrd7mxagDX4uOFC/Txvz/SsaNHJUm1o+ro0cceV8tWrd1cGbyZp3wV3IkTJ9S/f3+lpaUpKChIjRs31sqVK9WhQwdJ0pQpU+Tj46OePXvKZrOpU6dOevvtt52+jkesA1ivXj299NJL6tOnj8qXL6/du3erVq1aevHFF3X69Gm99ZZz636xDqD3uTu2ofILCpSSelIWWdSvawsNj2un23u/qp8OpWvZ2wkKLu+n4a8u0m8ZWerV+RaNHnKPYh6aqN37f3V3+ShGrAPo/datXaNSpUqpemSkDMPQss+Was77s/TvT5YoKqqOu8uDC7lzHcAaw5a77NxHpnVx2bmvlUfMAUxNTdUdd9whSfLz89PZs2clSf/4xz/00UcfubM0eIgVG/Zq5cYfdTD1pFJST2jM9GXKyrHptsY1JUm3N6mltxeu1/Yf/qcjR0/ptfdWKuPsOTVrUM3NlQNwVpu2d6pVbGtFRtZQjRo19cSw4SpXrpz27N7l7tLgxTx5DqAreEQDGBYWptOnT0uSqlevri1btkiSDh8+LA8IKOFhfHwseqDTzfL389XWPYclSVt2H9L9HW9WhcByslj+fL+stbQ2bD/g5moBXI/8/Hx9seJznTuXoyZNmrm7HHgzFy4E7Yk8Yg7gnXfeqf/85z9q1qyZBgwYoOHDh2vx4sXavn37VReMttlshVbTNgryZfEp5cqS4QY3RUVo3dwRKutbWlnnbOo14l3tO/Tnwpf9nn5f8157WMfWT9T58/nKyc1Tr8R3deiX39xcNYBrceDn/fpH397Ky7OpXLlymvLGdNX+v2UxAFw/j5gDWFBQoIKCApUu/Wc/unDhQm3atEl16tTRo48+Kl9f38t+dsyYMYVW1y5V5VaVCb/NpTWj5JUpXUrVwisoKMBP97Vvpvj7otVx0DTtO5Suyc88oFtuitSLb/1HpzKy1bVNYz3Rr63aPzxVP6Qcc3fpKEbMATSH83l5SktLU1bWWa36aqWWfLJIs+Z8SBPo5dw5B7BW4gqXnfvQ5Ltddu5r5REN4PW4VAJYudUzJIAm8PnMoTr0y2+aPPdr/bhsjJr3HK+fDqU7vH/wl9/05ISFbqwSxY0G0JweGRivqtWq68Ux49xdClyIBrDkeMQcQEn65ptv1K9fP0VHR+vo/z36P2/ePG3cuPGKn7NarQoMDHTYaP7MwcdikdW3tMqV/TMhLrjo3zL5+YZ8PHTyLQDnFBQU6PxFa8QCxYmHQNzgk08+UadOneTn56edO3faE73MzEy98sorbq4OnmDcE/cqpnltVQ8P0U1RERr3xL2KvaWOFq7Yrv1H0pWSekJvvdBHt9wUqZpVK2nYP+5Uu9vratm63e4uHYCTpk2ZpB3bt+no0V914Of9mjZlkrZv+6/u7tLV3aUBXsMjHgIZP368Zs6cqf79+2vhwv8/XBcTE6Px48e7sTJ4itCQAM16ub/CKgUqMytXew8cVdfH39aarfskSd2fmKHxT3bT4mmPKqCcVQd/OalBL87Tyo0/urlyAM46ffqUXkh6RidPnlBA+fK68ca6mvGvWYq+I8bdpcGLeWhQ5zIe0QDu379fsbGxhfYHBQUpIyOj5AuCx3ls7IIrvn8w9aT6jHyvhKoB4EpjX2bkB3A1jxgCDgsLU0pKSqH9GzduVK1atdxQEQAAMBPmALrB4MGDNWzYMG3dulUWi0XHjh3T/PnzNWLECD322GPuLg8AAHg5i8V1myfyiCHgZ599VgUFBWrXrp1ycnIUGxsrq9WqUaNGadCgQe4uDwAAwKt4RAJosVj0/PPP6/Tp09q7d6+2bNmikydPKigoSDVr1nR3eQAAwMsxBFyCbDabkpKSdMsttygmJkYrVqxQgwYN9MMPP6hu3bqaNm2ahg8f7s4SAQAAvI5bh4BffPFFvfPOO2rfvr02bdqkBx54QAMGDNCWLVs0adIkPfDAAypVikWdAQCAa3loUOcybm0AFy1apA8++ED33nuv9u7dq8aNG+uPP/7Q7t27PTYyBQAA+LtzawP466+/6uabb5YkNWzYUFarVcOHD6f5AwAAJcrHx1y9h1vnAObn58vX19f+unTp0goICHBjRQAAAN7PrQmgYRiKj4+X1WqVJOXm5mrIkCHy9/d3OO7TTz91R3kAAMAkzDb46NYGMC4uzuF1v3793FQJAAAwM7NNP3NrAzh79mx3Xh4AAMCUPOKbQAAAANzJZAGgZ3wTCAAAAEoOCSAAADA9s80BJAEEAAAwGRJAAABgeiSAAAAA8GokgAAAwPRMFgDSAAIAADAEDAAAAK9GAggAAEzPZAEgCSAAAIDZkAACAADTYw4gAAAAvBoJIAAAMD2TBYAkgAAAAGZDAggAAEyPOYAAAADwaiSAAADA9EwWANIAAgAAMAQMAAAAr0YCCAAATM9kASAJIAAAgNmQAAIAANNjDiAAAAC8GgkgAAAwPZMFgCSAAAAAZkMCCAAATM9scwBpAAEAgOmZrP9jCBgAAMBsSAABAIDpmW0ImAQQAADAZEgAAQCA6ZEAAgAAwKuRAAIAANMzWQBIAggAAOApkpOTdeutt6p8+fKqXLmyunfvrv379zsc06ZNG1ksFodtyJAhTl2HBhAAAJjexQ1VcW7OWL9+vRISErRlyxatWrVK58+fV8eOHZWdne1w3ODBg5WWlmbfJk6c6NR1GAIGAACm5ylDwF9++aXD6zlz5qhy5crasWOHYmNj7fvLlSunsLCwa74OCSAAAIAL2Ww2nTlzxmGz2WxF+mxmZqYkKSQkxGH//PnzValSJTVs2FBJSUnKyclxqiYaQAAAYHquHAJOTk5WUFCQw5acnHzVmgoKCvTUU08pJiZGDRs2tO/v27evPvzwQ61du1ZJSUmaN2+e+vXr59T9MgQMAADgQklJSUpMTHTYZ7Var/q5hIQE7d27Vxs3bnTY/8gjj9j/v1GjRgoPD1e7du108OBB1a5du0g10QACAADTc+UcQKvVWqSG76+GDh2q5cuXa8OGDapateoVj23RooUkKSUlhQYQAADg78YwDD3xxBNasmSJ1q1bp5o1a171M7t27ZIkhYeHF/k6NIAAAMD0fDzkMeCEhAQtWLBAn332mcqXL6/09HRJUlBQkPz8/HTw4EEtWLBAd999typWrKg9e/Zo+PDhio2NVePGjYt8HRpAAAAADzFjxgxJfy72/FezZ89WfHy8fH199fXXX2vq1KnKzs5WtWrV1LNnT73wwgtOXYcGEAAAmJ6HBIAyDOOK71erVk3r16+/7uvQAAIAANNz9hs7/u5YBxAAAMBkSAABAIDp+ZgrACQBBAAAMBsSQAAAYHrMAQQAAIBXIwEEAACmZ7IAkAQQAADAbEgAAQCA6VlkrgiQBhAAAJgey8AAAADAq5EAAgAA02MZGAAAAHg1EkAAAGB6JgsASQABAADMhgQQAACYno/JIkASQAAAAJMhAQQAAKZnsgCQBhAAAMBsy8AUqQHcs2dPkU/YuHHjay4GAAAArlekBrBp06ayWCwyDOOS7194z2KxKD8/v1gLBAAAcDWTBYBFawAPHz7s6joAAABQQorUAEZGRrq6DgAAALdhGZgimDdvnmJiYhQREaH//e9/kqSpU6fqs88+K9biAAAAUPycbgBnzJihxMRE3X333crIyLDP+QsODtbUqVOLuz4AAACXs7hw80RON4Bvvvmm3n33XT3//PMqVaqUff8tt9yi77//vliLAwAAQPFzeh3Aw4cPq1mzZoX2W61WZWdnF0tRAAAAJcls6wA6nQDWrFlTu3btKrT/yy+/VP369YujJgAAgBLlY3Hd5omcTgATExOVkJCg3NxcGYah//73v/roo4+UnJys9957zxU1AgAAoBg53QAOGjRIfn5+euGFF5STk6O+ffsqIiJC06ZNU+/evV1RIwAAgEuZbQj4mr4L+KGHHtJDDz2knJwcZWVlqXLlysVdFwAAAFzkmhpASTpx4oT2798v6c+uOTQ0tNiKAgAAKEkmCwCdfwjk7Nmz+sc//qGIiAi1bt1arVu3VkREhPr166fMzExX1AgAAIBi5HQDOGjQIG3dulWff/65MjIylJGRoeXLl2v79u169NFHXVEjAACAS1ksFpdtnsjpIeDly5dr5cqVatmypX1fp06d9O677+quu+4q1uIAAABQ/JxuACtWrKigoKBC+4OCglShQoViKQoAAKAkeep6fa7i9BDwCy+8oMTERKWnp9v3paena9SoURo9enSxFgcAAFASGAK+hGbNmjncwIEDB1S9enVVr15dkpSamiqr1aqTJ08yDxAAAMDDFakB7N69u4vLAAAAcB/PzOlcp0gN4EsvveTqOgAAAFBCrnkhaAAAAG/h46Fz9VzF6QYwPz9fU6ZM0ccff6zU1FTl5eU5vH/69OliKw4AAADFz+mngMeOHavJkyerV69eyszMVGJionr06CEfHx+NGTPGBSUCAAC4lsXius0TOd0Azp8/X++++65GjBih0qVLq0+fPnrvvff04osvasuWLa6oEQAAAMXI6QYwPT1djRo1kiQFBATYv/+3S5cu+vzzz4u3OgAAgBJgtnUAnW4Aq1atqrS0NElS7dq19dVXX0mStm3bJqvVWrzVAQAAoNg53QDed999Wr16tSTpiSee0OjRo1WnTh31799fDz/8cLEXCAAA4GpmmwPo9FPAr776qv3/e/XqpcjISG3atEl16tRR165di7U4AACAkmC2ZWCcTgAvdvvttysxMVEtWrTQK6+8Uhw1AQAAwIWuuwG8IC0tTaNHjy6u0wEAAJQYsw0BF1sDCAAAgL8HvgoOAACYnqcu1+IqJIAAAAAmU+QEMDEx8Yrvnzx58rqLKS5Bt7RxdwkAXGT/sbPuLgGAizSpXt5t1zZbIlbkBnDnzp1XPSY2Nva6igEAAIDrFbkBXLt2rSvrAAAAcBtPmQOYnJysTz/9VPv27ZOfn5/uuOMOvfbaa6pbt679mNzcXI0YMUILFy6UzWZTp06d9Pbbb6tKlSpFvo7ZEk8AAIBCfCyu25yxfv16JSQkaMuWLVq1apXOnz+vjh07Kjs7237M8OHDtWzZMi1atEjr16/XsWPH1KNHD6euw1PAAAAAHuLLL790eD1nzhxVrlxZO3bsUGxsrDIzMzVr1iwtWLBAd955pyRp9uzZql+/vrZs2aLbb7+9SNehAQQAAKbnbFLnDJvNJpvN5rDParXKarVe9bOZmZmSpJCQEEnSjh07dP78ebVv395+TL169VS9enVt3ry5yA0gQ8AAAAAulJycrKCgIIctOTn5qp8rKCjQU089pZiYGDVs2FCSlJ6eLl9fXwUHBzscW6VKFaWnpxe5JhJAAABgeq58CCQpKanQcnpFSf8SEhK0d+9ebdy4sdhruqYE8JtvvlG/fv0UHR2to0ePSpLmzZvnkgIBAAD+zqxWqwIDAx22qzWAQ4cO1fLly7V27VpVrVrVvj8sLEx5eXnKyMhwOP748eMKCwsrck1ON4CffPKJOnXqJD8/P+3cudM+pp2ZmalXXnnF2dMBAAC4nac8BWwYhoYOHaolS5ZozZo1qlmzpsP7N998s8qUKaPVq1fb9+3fv1+pqamKjo4u+v06V5Y0fvx4zZw5U++++67KlClj3x8TE6PvvvvO2dMBAADg/yQkJOjDDz/UggULVL58eaWnpys9PV3nzp2TJAUFBWngwIFKTEzU2rVrtWPHDg0YMEDR0dFFfgBEuoY5gPv377/kN34EBQUViiMBAAD+DjxkHWjNmDFDktSmTRuH/bNnz1Z8fLwkacqUKfLx8VHPnj0dFoJ2htMNYFhYmFJSUlSjRg2H/Rs3blStWrWcPR0AAIDb+XhIB2gYxlWPKVu2rKZPn67p06df83WcHgIePHiwhg0bpq1bt8pisejYsWOaP3++Ro4cqccee+yaCwEAAEDJcDoBfPbZZ1VQUKB27dopJydHsbGxslqtGjlypJ544glX1AgAAOBSZlsY2ekG0GKx6Pnnn9eoUaOUkpKirKwsNWjQQAEBAa6oDwAAAMXsmheC9vX1VYMGDYqzFgAAALfwkCmAJcbpBrBt27ZXXC17zZo111UQAAAAXMvpBrBp06YOr8+fP69du3Zp7969iouLK666AAAASoynPAVcUpxuAKdMmXLJ/WPGjFFWVtZ1FwQAAADXKraHXvr166f333+/uE4HAABQYiwW122e6JofArnY5s2bVbZs2eI6HQAAQIlx9jt7/+6cbgB79Ojh8NowDKWlpWn79u0aPXp0sRUGAAAA13C6AQwKCnJ47ePjo7p162rcuHHq2LFjsRUGAABQUngI5Ary8/M1YMAANWrUSBUqVHBVTQAAAHAhpx4CKVWqlDp27KiMjAwXlQMAAFDyzPYQiNNPATds2FCHDh1yRS0AAAAoAU43gOPHj9fIkSO1fPlypaWl6cyZMw4bAADA342PxXWbJyryHMBx48ZpxIgRuvvuuyVJ9957r8NXwhmGIYvFovz8/OKvEgAAAMWmyA3g2LFjNWTIEK1du9aV9QAAAJQ4izw0qnORIjeAhmFIklq3bu2yYgAAANzBU4dqXcWpOYAWT32UBQAAAEXm1DqAN95441WbwNOnT19XQQAAACXNbAmgUw3g2LFjC30TCAAAAP5enGoAe/furcqVK7uqFgAAALcw2zS3Is8BNNsPBgAAwFs5/RQwAACAt2EO4GUUFBS4sg4AAACUEKfmAAIAAHgjs810owEEAACm52OyDtCphaABAADw90cCCAAATM9sD4GQAAIAAJgMCSAAADA9k00BJAEEAAAwGxJAAABgej4yVwRIAggAAGAyJIAAAMD0zDYHkAYQAACYHsvAAAAAwKuRAAIAANPjq+AAAADg1UgAAQCA6ZksACQBBAAAMBsSQAAAYHrMAQQAAIBXIwEEAACmZ7IAkAYQAADAbEOiZrtfAAAA0yMBBAAApmcx2RgwCSAAAIDJkAACAADTM1f+RwIIAABgOiSAAADA9FgIGgAAAF6NBBAAAJieufI/GkAAAADTfRMIQ8AAAAAmQwMIAABMz2KxuGxz1oYNG9S1a1dFRETIYrFo6dKlDu/Hx8cXusZdd93l1DVoAAEAADxIdna2mjRpounTp1/2mLvuuktpaWn27aOPPnLqGswBBAAApudJiVjnzp3VuXPnKx5jtVoVFhZ2zdfwpPsFAADwOjabTWfOnHHYbDbbdZ1z3bp1qly5surWravHHntMp06dcurzNIAAAMD0XDkHMDk5WUFBQQ5bcnLyNdd611136YMPPtDq1av12muvaf369ercubPy8/OLfA6GgAEAAFwoKSlJiYmJDvusVus1n6937972/2/UqJEaN26s2rVra926dWrXrl2RzkECCAAATM/iws1qtSowMNBhu54G8GK1atVSpUqVlJKSUuTP0AACAAD8jf366686deqUwsPDi/wZhoABAIDpXct6fa6SlZXlkOYdPnxYu3btUkhIiEJCQjR27Fj17NlTYWFhOnjwoJ5++mlFRUWpU6dORb4GDSAAADA9TxoS3b59u9q2bWt/fWH+YFxcnGbMmKE9e/Zo7ty5ysjIUEREhDp27KiXX37ZqWFlGkAAAAAP0qZNGxmGcdn3V65ced3XoAEEAACm50lDwCXBkxJPAAAAlAASQAAAYHrmyv9IAAEAAEyHBBAAAJieyaYAkgACAACYDQkgAAAwPR+TzQKkAQQAAKbHEDAAAAC8GgkgAAAwPYvJhoBJAAEAAEyGBBAAAJgecwABAADg1UgAAQCA6ZltGRgSQAAAAJMhAQQAAKZntjmANIAAAMD0zNYAMgQMAABgMiSAAADA9FgIGgAAAF6NBBAAAJiej7kCQBJAAAAAsyEBBAAApsccQAAAAHg1EkAAAGB6rAPoJt9884369eun6OhoHT16VJI0b948bdy40c2VAQAAb2dx4X+eyCMawE8++USdOnWSn5+fdu7cKZvNJknKzMzUK6+84ubqAAAAvItHNIDjx4/XzJkz9e6776pMmTL2/TExMfruu+/cWBkAADADH4vrNk/kEQ3g/v37FRsbW2h/UFCQMjIySr4gAAAAL+YRDWBYWJhSUlIK7d+4caNq1arlhooAAICZMAfQDQYPHqxhw4Zp69atslgsOnbsmObPn6+RI0fqsccec3d5AAAAXsUjloF59tlnVVBQoHbt2iknJ0exsbGyWq0aOXKknnjiCXeXBw8Q17qW4trUUrWK/pKk/cfOaPLyn7Rmb7okqV+rmurRoroaVQ9Web8yuvHJz3Tm3Hl3lgygiH7c853+s2ieDv/8k34//ZtGjvmnbotpY3/fMAx9PPcdrf5iibKzslTvpiYa9OSzCq9a3X1Fw+uwDIwb/PHHH3r++ed1+vRp7d27V1u2bNHJkyf18ssv67fffnN3efAAx34/pwmf7FXH8avVacJqbdx3QnMS7lDdiEBJkp9vKa3Zm65pK/a5uVIAzrLlnlONWnU08IlnLvn+Z/+eqy+WLtTgYUl65c05spYtqwlJTygvz1bClQLewyMSwN69e2vx4sXy9fVVgwYN7PuPHz+udu3aae/evW6sDp5g1Z40h9evLv1BcW1qq3mtEO0/dkbvrv5zDukdN4a6ozwA16HZbTFqdlvMJd8zDEMrlnykHg8N1K13tJEkDX1mnAY/0FHbvl2nmLadSrBSeDOTBYCekQCmpqZq0KBBDvvS0tLUpk0b1atXz01VwVP5WKRut1ZVOd9S2nHwlLvLAeBCJ9KPKuP0KTVudpt9Xzn/AEXVa6iff/zejZXB2/hYLC7bPJFHJIArVqxQbGysEhMTNXnyZB07dkxt27ZVkyZNtHDhwit+1maz2ReOvsDIPy9LqTKX+QT+rurdEKjPn71T1jI+yrb9oYff3qyf0866uywALpRx+s9/5AVVqOiwP6hCiDJ+5x+AwLXyiAYwNDRUX331lVq2bClJWr58uZo3b6758+fLx+fKIWVycrLGjh3rsM+/2QMKuPlBl9UL9ziYflbtxq1SoF8Zdbm5qt54+Fbd9/o6mkAAwHXzzJzOdTxiCFiSqlWrplWrVmn+/Pm67bbb9NFHH6lUqVJX/VxSUpIyMzMdNv+m95VAxShp5/MNHTmZrT2pGXplyV798EuGBrWr4+6yALhQcMifyV/mRWlf5u+nFXxRKgig6NyWAFaoUEGWS4yL5+TkaNmyZapY8f//Yp8+ffqy57FarbJarQ77GP41Bx8fi6xlPObfMABcoHLYDQoOqajvd25Tjai6kqSc7Cyl7Nurjl17urk6eBWTRYBuawCnTp3qrkvjb+i5+xpqzd50HT2dI/+ypdXjtuq648ZQ9Z76jSQpNNCqykFlVaPyn+sE1q8apKzc8zp6KkcZOawHCHiy3HM5Sj/6i/31ifSjOpKyXwGBQapUOUx339dHny6YpfAbqqly+A1aOGeGKlQM1a1/WSsQgHMshmEY7i6iuIUNXuzuElDMJsfdrFb1KqtyUFmdPXdeP/6aqbe+3K8NP52QJI3s2kAj721Q6HPDZm/Tvzf9r6TLhQutHM2yH97mh93bNXbkkEL7W3foooSnx9gXgv56xRLlZJ1VvYZNNfDJZxRRNdIN1cKVmlQv77Zrbz2Y6bJzt6gd5LJzXyuPawBzc3OVl5fnsC8wMNCpc9AAAt6LBhDwXjSAJccjJlBlZ2dr6NChqly5svz9/VWhQgWHDQAAwJUsFtdtnsgjGsCnn35aa9as0YwZM2S1WvXee+9p7NixioiI0AcffODu8gAAgJezuHDzRB6xDuCyZcv0wQcfqE2bNhowYIBatWqlqKgoRUZGav78+XrooYfcXSIAAIDX8IgE8PTp06pVq5akP+f7XVj2pWXLltqwYYM7SwMAAGZgsgjQIxrAWrVq6fDhw5KkevXq6eOPP5b0ZzIYHBzsxsoAAAC8j1sbwEOHDqmgoEADBgzQ7t27JUnPPvuspk+frrJly2r48OEaNWqUO0sEAAAmYHHhf57IrXMA69Spo7S0NA0fPlyS1KtXL73xxhvat2+fduzYoaioKDVu3NidJQIAAHgdtyaAFy9BuGLFCmVnZysyMlI9evSg+QMAACWCZWAAAADg1dw6BGyxWGS5qDW++DUAAICrma37cGsDaBiG4uPjZbVaJf35NXBDhgyRv7+/w3GffvqpO8oDAABmYbIO0K0NYFxcnMPrfv36uakSAAAA83BrAzh79mx3Xh4AAECSPHa5FlfhIRAAAAAPsmHDBnXt2lURERGyWCxaunSpw/uGYejFF19UeHi4/Pz81L59ex04cMCpa9AAAgAA0/OkZWCys7PVpEkTTZ8+/ZLvT5w4UW+88YZmzpyprVu3yt/fX506dVJubm6Rr+HWIWAAAAA46ty5szp37nzJ9wzD0NSpU/XCCy+oW7dukqQPPvhAVapU0dKlS9W7d+8iXYMEEAAAmJ7FhZvNZtOZM2ccNpvNdk11Hj58WOnp6Wrfvr19X1BQkFq0aKHNmzcX+Tw0gAAAAC6UnJysoKAghy05OfmazpWeni5JqlKlisP+KlWq2N8rCoaAAQAAXPgQcFJSkhITEx32XVgD2V1oAAEAgOm5chkYq9VabA1fWFiYJOn48eMKDw+37z9+/LiaNm1a5PMwBAwAAPA3UbNmTYWFhWn16tX2fWfOnNHWrVsVHR1d5POQAAIAANO7luVaXCUrK0spKSn214cPH9auXbsUEhKi6tWr66mnntL48eNVp04d1axZU6NHj1ZERIS6d+9e5GvQAAIAAHiQ7du3q23btvbXF+YPxsXFac6cOXr66aeVnZ2tRx55RBkZGWrZsqW+/PJLlS1btsjXsBiGYRR75W4WNnixu0sA4CIrR3dydwkAXKRJ9fJuu/beX7Ncdu6GVQNcdu5rxRxAAAAAk2EIGAAAwIPmAJYEEkAAAACTIQEEAACm58p1AD0RCSAAAIDJkAACAADT86R1AEsCDSAAADA9k/V/DAEDAACYDQkgAACAySJAEkAAAACTIQEEAACmxzIwAAAA8GokgAAAwPTMtgwMCSAAAIDJkAACAADTM1kASAMIAABgtg6QIWAAAACTIQEEAACmxzIwAAAA8GokgAAAwPRYBgYAAABejQQQAACYnskCQBJAAAAAsyEBBAAAMFkESAMIAABMj2VgAAAA4NVIAAEAgOmxDAwAAAC8GgkgAAAwPZMFgCSAAAAAZkMCCAAAYLIIkAQQAADAZEgAAQCA6ZltHUAaQAAAYHosAwMAAACvRgIIAABMz2QBIAkgAACA2ZAAAgAA02MOIAAAALwaCSAAAIDJZgGSAAIAAJgMCSAAADA9s80BpAEEAACmZ7L+jyFgAAAAsyEBBAAApme2IWASQAAAAJMhAQQAAKZnMdksQBJAAAAAkyEBBAAAMFcASAIIAABgNiSAAADA9EwWANIAAgAAsAwMAAAAvBoJIAAAMD2WgQEAAIBXowEEAACwuHBzwpgxY2SxWBy2evXqXe/dFcIQMAAAgAe56aab9PXXX9tfly5d/O0aDSAAADA9T5oBWLp0aYWFhbn0GgwBAwAAuJDNZtOZM2ccNpvNdtnjDxw4oIiICNWqVUsPPfSQUlNTi70mGkAAAGB6FovrtuTkZAUFBTlsycnJl6yjRYsWmjNnjr788kvNmDFDhw8fVqtWrXT27NnivV/DMIxiPaMHCBu82N0lAHCRlaM7ubsEAC7SpHp5t137dHa+y87tX/qPQomf1WqV1Wq96mczMjIUGRmpyZMna+DAgcVWE3MAAQAAXKiozd6lBAcH68Ybb1RKSkqx1sQQMAAAMD1XDgFfj6ysLB08eFDh4eHFc6P/hwYQAADAQ4wcOVLr16/XkSNHtGnTJt13330qVaqU+vTpU6zXYQgYAADAQ/z666/q06ePTp06pdDQULVs2VJbtmxRaGhosV6HBhAAAMBDLFy4sESuQwMIAABM73rn6v3dMAcQAADAZEgAAQCA6Vk86svgXI8GEAAAmB5DwAAAAPBqJIAAAMD0TBYAkgACAACYDQkgAACAySJAEkAAAACTIQEEAACmZ7ZlYEgAAQAATIYEEAAAmB7rAAIAAMCrkQACAADTM1kASAMIAABgtg6QIWAAAACTIQEEAACmxzIwAAAA8GokgAAAwPRYBgYAAABezWIYhuHuIoBrZbPZlJycrKSkJFmtVneXA6AY8fsNuA4NIP7Wzpw5o6CgIGVmZiowMNDd5QAoRvx+A67DEDAAAIDJ0AACAACYDA0gAACAydAA4m/NarXqpZdeYoI44IX4/QZch4dAAAAATIYEEAAAwGRoAAEAAEyGBhAAAMBkaABhOvHx8erevbu7ywBQBHPmzFFwcLC7ywC8Dg0gPEp8fLwsFossFovKlCmjmjVr6umnn1Zubq67SwNwHf76u/3XLSUlxd2lAaZU2t0FABe76667NHv2bJ0/f147duxQXFycLBaLXnvtNXeXBuA6XPjd/qvQ0FA3VQOYGwkgPI7ValVYWJiqVaum7t27q3379lq1apUkqaCgQMnJyapZs6b8/PzUpEkTLV682P7Z/Px8DRw40P5+3bp1NW3aNHfdCoC/uPC7/ddt2rRpatSokfz9/VWtWjU9/vjjysrKuuw5Tp48qVtuuUX33XefbDbbVf8mALg0EkB4tL1792rTpk2KjIyUJCUnJ+vDDz/UzJkzVadOHW3YsEH9+vVTaGioWrdurYKCAlWtWlWLFi1SxYoVtWnTJj3yyCMKDw/Xgw8+6Oa7AXAxHx8fvfHGG6pZs6YOHTqkxx9/XE8//bTefvvtQsf+8ssv6tChg26//XbNmjVLpUqV0oQJE674NwHAZRiAB4mLizNKlSpl+Pv7G1ar1ZBk+Pj4GIsXLzZyc3ONcuXKGZs2bXL4zMCBA40+ffpc9pwJCQlGz549Ha7RrVs3V90CgEv46+/2he3+++8vdNyiRYuMihUr2l/Pnj3bCAoKMvbt22dUq1bNePLJJ42CggLDMIxr/psAwDBIAOFx2rZtqxkzZig7O1tTpkxR6dKl1bNnT/3www/KyclRhw4dHI7Py8tTs2bN7K+nT5+u999/X6mpqTp37pzy8vLUtGnTEr4LABe78Lt9gb+/v77++mslJydr3759OnPmjP744w/l5uYqJydH5cqVkySdO3dOrVq1Ut++fTV16lT751NSUor0NwFAYTSA8Dj+/v6KioqSJL3//vtq0qSJZs2apYYNG0qSPv/8c91www0On7nwXaELFy7UyJEjNWnSJEVHR6t8+fJ6/fXXtXXr1pK9CQCF/PV3W5KOHDmiLl266LHHHtOECRMUEhKijRs3auDAgcrLy7M3gFarVe3bt9fy5cs1atQo++//hbmCV/qbAODSaADh0Xx8fPTcc88pMTFRP//8s6xWq1JTUy87t+fbb7/VHXfcoccff9y+7+DBgyVVLgAn7NixQwUFBZo0aZJ8fP58JvHjjz8udJyPj4/mzZunvn37qm3btlq3bp0iIiLUoEGDq/5NAHBpNIDweA888IBGjRqld955RyNHjtTw4cNVUFCgli1bKjMzU99++60CAwMVFxenOnXq6IMPPtDKlStVs2ZNzZs3T9u2bVPNmjXdfRsALhIVFaXz58/rzTffVNeuXfXtt99q5syZlzy2VKlSmj9/vvr06aM777xT69atU1hY2FX/JgC4NBpAeLzSpUtr6NChmjhxog4fPqzQ0FAlJyfr0KFDCg4OVvPmzfXcc89Jkh599FHt3LlTvXr1ksViUZ8+ffT444/riy++cPNdALhYkyZNNHnyZL322mtKSkpSbGyskpOT1b9//0seX7p0aX300Ufq1auXvQl8+eWXr/g3AcClWQzDMNxdBAAAAEoOC0EDAACYDA0gAACAydAAAgAAmAwNIAAAgMnQAAIAAJgMDSAAAIDJ0AACAACYDA0gAACAydAAAig28fHx6t69u/11mzZt9NRTT5V4HevWrZPFYlFGRobLrnHxvV6LkqgTAC6FBhDwcvHx8bJYLLJYLPL19VVUVJTGjRunP/74w+XX/vTTT/Xyyy8X6diSboZq1KihqVOnlsi1AMDT8F3AgAncddddmj17tmw2m1asWKGEhASVKVNGSUlJhY7Ny8uTr69vsVw3JCSkWM4DACheJICACVitVoWFhSkyMlKPPfaY2rdvr//85z+S/v9Q5oQJExQREaG6detKkn755Rc9+OCDCg4OVkhIiLp166YjR47Yz5mfn6/ExEQFBwerYsWKevrpp3XxV4tfPARss9n0zDPPqFq1arJarYqKitKsWbN05MgRtW3bVpJUoUIFWSwWxcfHS5IKCgqUnJysmjVrys/PT02aNNHixYsdrrNixQrdeOON8vPzU9u2bR3qvBb5+fkaOHCg/Zp169bVtGnTLnns2LFjFRoaqsDAQA0ZMkR5eXn294pSOwC4AwkgYEJ+fn46deqU/fXq1asVGBioVatWSZLOnz+vTp06KTo6Wt98841Kly6t8ePH66677tKePXvk6+urSZMmac6cOXr//fdVv359TZo0SUuWLNGdd9552ev2799fmzdv1htvvKEmTZro8OHD+u2331StWjV98skn6tmzp/bv36/AwED5+flJkpKTk/Xhhx9q5syZqlOnjjZs2KB+/fopNDRUrVu31i+//KIePXooISFBjzzyiLZv364RI0Zc18+noKBAVatW1aJFi1SxYkVt2rRJjzzyiMLDw/Xggw86/NzKli2rdevW6ciRIxowYIAqVqyoCRMmFKl2AHAbA4BXi4uLM7p162YYhmEUFBQYq1atMqxWqzFy5Ej7+1WqVDFsNpv9M/PmzTPq1q1rFBQU2PfZbDbDz8/PWLlypWEYhhEeHm5MnDjR/v758+eNqlWr2q9lGIbRunVrY9iwYYZhGMb+/fsNScaqVasuWefatWsNScbvv/9u35ebm2uUK1fO2LRpk8OxAwcONPr06WMYhmEkJSUZDRo0cHj/mWeeKXSui0VGRhpTpky57PsXS0hIMHr27Gl/HRcXZ4SEhBjZ2dn2fTNmzDACAgKM/Pz8ItV+qXsGgJJAAgiYwPLlyxUQEKDz58+roKBAffv21ZgxY+zvN2rUyGHe3+7du5WSkqLy5cs7nCc3N1cHDx5UZmam0tLS1KJFC/t7pUuX1i233FJoGPiCXbt2qVSpUk4lXykpKcrJyVGHDh0c9ufl5alZs2aSpJ9++smhDkmKjo4u8jUuZ/r06Xr//feVmpqqc+fOKS8vT02bNnU4pkmTJipXrpzDdbOysvTLL78oKyvrqrUDgLvQAAIm0LZtW82YMUO+vr6KiIhQ6dKOv/r+/v4Or7OysnTzzTdr/vz5hc4VGhp6TTVcGNJ1RlZWliTp888/1w033ODwntVqvaY6imLhwoUaOXKkJk2apOjoaJUvX16vv/66tm7dWuRzuKt2ACgKGkDABPz9/RUVFVXk45s3b65///vfqly5sgIDAy95THh4uLZu3arY2FhJ0h9//KEdO3aoefPmlzy+UaNGKigo0Pr169W+fftC719IIPPz8+37GjRoIKvVqtTU1Msmh/Xr17c/0HLBli1brn6TV/Dtt9/qjjvu0OOPP27fd/DgwULH7d69W+fOnbM3t1u2bFFAQICqVaumkJCQq9YOAO7CU8AACnnooYdUqVIldevWTd98840OHz6sdevW6cknn9Svv/4qSRo2bJheffVVLV26VPv27dPjjz9+xTX8atSoobi4OD388MNaunSp/Zwff/yxJCkyMlIWi0XLly/XyZMnlZWVpfLly2vkyJEaPny45s6dq4MHD+q7777Tm2++qblz50qShgwZogMHDmjUqFHav3+/FixYoDlz5hTpPo8ePapdu3Y5bL///rvq1Kmj7du3a+XKlfr55581evRobdu2rdDn8/LyNHDgQP34449asWKFXnrpJQ0dOlQ+Pj5Fqh0A3MbdkxABuNZfHwJx5v20tDSjf//+RqVKlQyr1WrUqlXLGDx4sJGZmWkYxp8PfQwbNswIDAw0goODjcTERKN///6XfQjEMAzj3LlzxvDhw43w8HDD19fXiIqKMt5//337++PGjTPCwsIMi8VixMXFGYbx54MrU6dONerWrWuUKVPGCA0NNTp16mSsX7/e/rlly5YZUVFRhtVqNVq1amW8//77RXoIRFKhbd68eUZubq4RHx9vBAUFGcHBwcZjjz1mPPvss0aTJk0K/dxefPFFo2LFikZAQIAxePBgIzc3137M1WrnIRAA7mIxjMvM2AYAAIBXYggYAADAZGgAAQAATIYGEAAAwGRoAAEAAEyGBhAAAMBkaAABAABMhgYQAADAZGgAAQAATIYGEAAAwGRoAAEAAEyGBhAAAMBk/h/41VAF1DFGbgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.55      0.93      0.69        41\n",
            "        Fake       0.77      0.24      0.37        41\n",
            "\n",
            "    accuracy                           0.59        82\n",
            "   macro avg       0.66      0.59      0.53        82\n",
            "weighted avg       0.66      0.59      0.53        82\n",
            "\n",
            "end time: 2024-11-06 21:28:00.385878\n",
            "executed in: 0:06:27.125677\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.8 GB\n",
            "Cached:    2.8 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-dcaada916080>:178: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs 14"
      ],
      "metadata": {
        "id": "DD-QV99dZfw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "base_path = 'deepfake-detection-challenge'\n",
        "\n",
        "#train_folder = os.listdir(str(sys.argv[1]))\n",
        "train_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "\n",
        "#test_folder = os.listdir(str(sys.argv[2]))\n",
        "test_folders = [os.path.join(\"/content/\", \"fake_videos\", \"train\")]\n",
        "\n",
        "batch_size = int(1)\n",
        "num_epochs = int(10)\n",
        "n_frames = int(30)\n",
        "lr = float(0.001)\n",
        "\n",
        "TRAIN_FOLDERS = train_folders\n",
        "TEST_FOLDERS = test_folders\n",
        "print(f\"all train folders: {train_folders}, {type(train_folders)}\")\n",
        "print(f\"all test folders: {test_folders}, {type(test_folders)}\")\n",
        "# AUTOENCODER = 'autoencoder_H10M46S22_04-11-21.pt'\n",
        "\n",
        "# batch_size = 10\n",
        "# num_epochs = 1\n",
        "# epoch_size = 500\n",
        "# n_frames = 30\n",
        "milestones = [6,12,18]\n",
        "gamma = 0.1\n",
        "n_vid_features = 36*36 # 3600\n",
        "n_aud_features = 1\n",
        "n_head = 8\n",
        "n_layers = 6\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#autoencoder = FaceAutoencoder()\n",
        "#if len(sys.argv) > 7:\n",
        "#    print(\"pretrained autoencoder is loaded\")\n",
        "#    AUTOENCODER = str(sys.argv[7])\n",
        "#    autoencoder.load_state_dict(torch.load(AUTOENCODER, map_location=device))\n",
        "#autoencoder.to(device)\n",
        "#autoencoder.eval()\n",
        "\n",
        "model = FaceClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/14_epochs_classifier_30_small_batch_8_dfdc.pt', map_location=device))\n",
        "\n",
        "model = model.to(device)\n",
        "class_weights = {0: 0.6191950464396285, 1: 2.5974025974025974}\n",
        "weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)\n",
        "\n",
        "# Modify the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(f'start time: {str(start_time)}')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "'''Splitting into Train and Validation'''\n",
        "train_dataset = FaceDeepfakeDataset(TRAIN_FOLDERS,  n_frames=n_frames, n_audio_reads=576, device=device, cache_folder=\"face_encode_cache\")\n",
        "test_dataset = FaceDeepfakeDataset(TEST_FOLDERS, n_frames=n_frames, n_audio_reads=576, device=device)\n",
        "# dataset_size = len(dataset)\n",
        "# val_split = .3\n",
        "# val_size = int(val_split * dataset_size)\n",
        "# train_size = dataset_size - val_size\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(len(train_loader))\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''Train_Loop'''\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_loss = np.inf\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_times = []\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_t_loss = 0\n",
        "    epoch_v_loss = 0\n",
        "    t_count = 0\n",
        "    t_count_wrong = 0\n",
        "    train_labels_all = []\n",
        "    train_preds_all = []\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_count = 0\n",
        "        v_count_wrong = 0\n",
        "        for i, batch in tqdm(enumerate(val_loader)):\n",
        "            # if i * batch_size >= epoch_size:\n",
        "        #        break\n",
        "            video_data, labels = batch\n",
        "            video_data = video_data.to(device)\n",
        "            #audio_data = audio_data.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(video_data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.round()\n",
        "            n_wrong = (labels - output).abs().sum()\n",
        "            v_count_wrong += n_wrong\n",
        "            v_count += labels.shape[0]\n",
        "\n",
        "            epoch_v_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            #print('.', end='', flush=True)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_exec_time = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_exec_time)\n",
        "    val_losses.append(epoch_t_loss/len(val_loader))\n",
        "\n",
        "    v_count_right = v_count - v_count_wrong\n",
        "    v_accuracy = v_count_right / v_count\n",
        "\n",
        "    val_accuracies.append(v_accuracy)\n",
        "\n",
        "    print(f'\\nepoch: {epoch}, val loss: {val_losses[-1]}, executed in: {str(epoch_exec_time)}')\n",
        "    #print(f\"train total: {t_count}, train correct: {t_count_right}, train incorrect: {t_count_wrong}, train accuracy: {t_accuracy}\")\n",
        "    print(f\"valid total: {v_count}, valid correct: {v_count_right}, valid incorrect: {v_count_wrong}, valid accuracy: {v_accuracy}\")\n",
        "    all_labels = np.array(all_labels).astype(int)\n",
        "    all_preds = np.array(all_preds).astype(int)\n",
        "    # Обчислення та візуалізація матриці плутанини\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    #print(conf_matrix)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Друк звіту про класифікацію\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(f\"end time: {str(end_time)}\")\n",
        "exec_time = end_time - start_time\n",
        "print(f\"executed in: {str(exec_time)}\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n"
      ],
      "metadata": {
        "id": "e5nJxB6EZivl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}